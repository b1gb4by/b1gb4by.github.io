<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>auto-scaling on A Life of Inquiry</title><link>https://example.com/tags/auto-scaling/</link><description>Recent content in auto-scaling on A Life of Inquiry</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 16 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://example.com/tags/auto-scaling/index.xml" rel="self" type="application/rss+xml"/><item><title>Karpenter Deep Dive</title><link>https://example.com/p/karpenter-deep-dive/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://example.com/p/karpenter-deep-dive/</guid><description>&lt;img src="https://example.com/p/karpenter-deep-dive/featured-image.webp" alt="Featured image of post Karpenter Deep Dive " />&lt;p>Recently, Karpenter graduated from AWS re:Invent with autoscaling of Nodes in Kubernetes clusters.&lt;/p>
&lt;p>In this article, we&amp;rsquo;ll take a deeper look at it.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This is the 18th day of the [Kubernetes Advent Calendar 2021](&amp;lt;(&lt;a class="link" href="https://qiita.com/advent-calendar/2021/kubernetes%29%29" target="_blank" rel="noopener"
>https://qiita.com/advent-calendar/2021/kubernetes))&lt;/a>.&lt;/p>
&lt;h2 id="what-is-karpenter">What is Karpenter?&lt;/h2>
&lt;p>Officially described as &amp;ldquo;Just-in-time Nodes for Any Kubernetes Cluster&amp;rdquo;, Karpenter provides the ability to instantly provision new Nodes for unscheduled Pods. The goal is to improve the efficiency and cost of running workloads on Kubernetes clusters.&lt;/p>
&lt;p>Karpenter works as follows.&lt;/p>
&lt;ul>
&lt;li>Monitor Pods that the Kubernetes scheduler has marked as unschedulable&lt;/li>
&lt;li>Evaluate the following scheduling constraints as requested by the Pod
&lt;ul>
&lt;li>Resource Request&lt;/li>
&lt;li>Node Selector&lt;/li>
&lt;li>Affinity&lt;/li>
&lt;li>Tolerant&lt;/li>
&lt;li>Topology spreading constraints&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Provisioning a Node to meet Pod requirements&lt;/li>
&lt;li>Scheduling a Pod to run on a new Node&lt;/li>
&lt;li>Deleting a Node when it is no longer needed&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>How to use Karpenter&lt;/p>
&lt;p>Karpenter will only support AWS as of December 2021.&lt;/p>
&lt;/blockquote>
&lt;h2 id="autoscale-in-kubernetes">Autoscale in Kubernetes&lt;/h2>
&lt;h3 id="pod">Pod&lt;/h3>
&lt;p>There are two ways to scale the Pod.&lt;/p>
&lt;h4 id="horizontal-pod-auto-scaler">Horizontal Pod Auto scaler&lt;/h4>
&lt;p>Horizontal pod scaling is a method of scaling to improve processing performance by increasing the number of pods. User-defined metrics such as CPU, memory, etc. can also be used to make decisions.&lt;/p>
&lt;p>The number of pods is calculated by the following formula.&lt;/p>
&lt;p>&lt;code>Number of replicas desired = ceil[&amp;lt;current number of pods&amp;gt; * (&amp;lt;current index value / &amp;lt;target index value&amp;gt;)]&lt;/code>.&lt;/p>
&lt;h4 id="vertical-pod-auto-scaler">Vertical Pod Auto scaler&lt;/h4>
&lt;p>This is a method of scaling that improves processing performance by increasing the resources available to the pod. In this case, the CPU and memory are used as criteria. It is more like optimizing the resource utilization.&lt;/p>
&lt;h3 id="node">Node&lt;/h3>
&lt;h4 id="cluster-auto-scaler">Cluster Auto scaler&lt;/h4>
&lt;p>It is a method of scaling to improve processing performance by increasing the number of worker Nodes. It can also be used in conjunction with horizontal scaling of Pods.&lt;/p>
&lt;h2 id="how-to-install">How to install&lt;/h2>
&lt;p>Karpenter will be installed on the cluster with Helm Chart.
Karpenter also requires IAM Roles for Service Accounts (IRSA).&lt;/p>
&lt;p>Currently, the utilities required to use Karpenter are as follows&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html" target="_blank" rel="noopener"
>AWS CLI&lt;/a>&lt;/li>
&lt;li>&lt;code>kubectl&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank" rel="noopener"
>the Kubernetes CLI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>eksctl&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html" target="_blank" rel="noopener"
>the CLI for AWS EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>helm&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://karpenter.sh/docs/getting-started/#install" target="_blank" rel="noopener"
>the package manager for Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>To learn how to install Karpenter on AWS, please refer to the official document &amp;ldquo;&lt;a class="link" href="https://karpenter.sh/docs/getting-started/#install" target="_blank" rel="noopener"
>Getting Started with Karpenter on AWS&lt;/a>&amp;rdquo;.&lt;/p>
&lt;p>Karpenter&amp;rsquo;s Helm Chart can be found &lt;a class="link" href="https://github.com/aws/karpenter/tree/main/charts/karpenter" target="_blank" rel="noopener"
>here&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>Installation with Terraform&lt;/p>
&lt;p>Kapenter also provides an installation method using &lt;a class="link" href="https://learn.hashicorp.com/tutorials/terraform/install-cli" target="_blank" rel="noopener"
>Terraform&lt;/a>. See &lt;a class="link" href="https://karpenter.sh/docs/getting-started-with-terraform/" target="_blank" rel="noopener"
>here&lt;/a> for details.&lt;/p>
&lt;/blockquote>
&lt;p>An overview diagram is shown in the figure below.&lt;/p>
&lt;p>&lt;img src="https://example.com/karpenter-overview.webp"
loading="lazy"
alt="Karpenter Overview"
>&lt;/p>
&lt;h2 id="configure-the-provisioner">Configure the provisioner&lt;/h2>
&lt;p>Karpenter&amp;rsquo;s job is to add Nodes that handle non-schedulable Pods, schedule Pods on those Nodes, and remove the Nodes when they are no longer needed.&lt;/p>
&lt;p>To configure Karpenter, create a provisioner that defines how Karpenter will manage non-schedulable Pods and timed Nodes.&lt;/p>
&lt;p>The following is what you need to know about Karpenter provisioners.&lt;/p>
&lt;h3 id="unschedulable-pods">Unschedulable pods&lt;/h3>
&lt;p>Karpenter will only attempt to provision Pods with the status condition &lt;code>Unschedulable=True&lt;/code>. This will be set when the kube-scheduler fails to schedule a Pod to an existing capacity.&lt;/p>
&lt;h3 id="provisioner-cr">Provisioner CR&lt;/h3>
&lt;p>Karpenter defines a custom resource called &lt;code>Provisioner&lt;/code> to specify the provisioning configuration.&lt;/p>
&lt;p>Each provisioner manages a separate set of Nodes, but a Pod can be scheduled to any provisioner that supports its scheduling constraints.&lt;/p>
&lt;p>A provisioner contains constraints that affect the Nodes that can be provisioned and the attributes of the Nodes (such as timers for removing Nodes).&lt;/p>
&lt;p>The following are the resources of the provisioner.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">karpenter.sh/v1alpha5&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Provisioner&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ttlSecondsUntilExpired&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2592000&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ttlSecondsAfterEmpty&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">30&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">taints&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">example.com/special-taint&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">effect&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">NoSchedule&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">billing-team&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">my-team&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requirements&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;node.kubernetes.io/instance-type&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;m5.large&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;m5.2xlarge&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;us-west-2a&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;us-west-2b&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;kubernetes.io/arch&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;arm64&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;amd64&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;karpenter.sh/capacity-type&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;spot&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;on-demand&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">provider&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>{}&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The &lt;code>spec.ttlSecondsUntilExpired&lt;/code> is the number of seconds the controller will wait before exiting the Node, measured from the time the Node is created. This is useful to eventually implement features like consistent Node upgrades, memory leak protection, and destructive testing. If this field is not set, termination due to expiration will be disabled.&lt;/p>
&lt;p>The &lt;code>spec.ttlSecondsAfterEmpty&lt;/code> is the number of seconds the controller will wait between the time it detects that a Node is empty and the time it tries to remove the Node. A Node is considered empty if there are no Pods scheduled for that Node, except for the daemonset.&lt;/p>
&lt;p>The &lt;code>spec.requirements&lt;/code> constrains the parameters of the provisioned Node. It can be combined with &lt;code>nodeAffinity&lt;/code> and &lt;code>nodeSelector&lt;/code>. The &lt;code>{ In, NotIn }&lt;/code> operator is supported to include or exclude values.&lt;/p>
&lt;h2 id="deprovisioning-node">Deprovisioning Node&lt;/h2>
&lt;p>Karpenter deletes the nodes that are no longer needed as follows.&lt;/p>
&lt;h3 id="finalizer">Finalizer&lt;/h3>
&lt;p>Karpenter will place a finalizer bit in each Node it creates.&lt;/p>
&lt;p>When a request to delete these Nodes comes in (such as a TTL or manual Node deletion via kubectl), Karpenter codes the Node, ejects all Pods, terminates the EC2 instance, and deletes the Node object.&lt;/p>
&lt;p>Karpenter handles all the cleanup work required to properly delete the Node.&lt;/p>
&lt;h3 id="node-expiry">Node Expiry&lt;/h3>
&lt;p>When a Node reaches its expiration value (&lt;code>ttlSecondsUntilExpired&lt;/code>), it will be ejected and removed from the Pod (even if it is still running a workload).&lt;/p>
&lt;h3 id="empty-nodes">Empty Nodes&lt;/h3>
&lt;p>When the last workload Pod running on a Karpenter-managed Node runs out, that Node will be given an &lt;code>emptiness&lt;/code> timestamp. When its &amp;ldquo;Node is empty&amp;rdquo; expiration date (&lt;code>ttlSecondsAfterEmpty&lt;/code>) is reached, finalization is triggered.&lt;/p>
&lt;blockquote>
&lt;p>About how to remove Node&lt;/p>
&lt;p>For more information on how Karpenter deletes Nodes, see &lt;a class="link" href="https://karpenter.sh/docs/tasks/deprov-nodes/" target="_blank" rel="noopener"
>Details&lt;/a> on Node deprovisioning.&lt;/p>
&lt;/blockquote>
&lt;h2 id="upgrade-node">Upgrade Node&lt;/h2>
&lt;p>An easy way to upgrade a Node is to set &lt;code>ttlSecondsUntilExpired&lt;/code>, which will expire after a set period of time and be replaced by a newer Node.&lt;/p>
&lt;h2 id="constraints">Constraints&lt;/h2>
&lt;p>Because there are no constraints defined by the provisioner or requested by the Pod being deployed, Karpenter is selected from the entire set of features available from the cloud provider. Nodes can be created using any instance type and run in any zone.&lt;/p>
&lt;h2 id="scheduling">Scheduling&lt;/h2>
&lt;p>Karpenter schedules Pods that are marked as &lt;code>unschedulable&lt;/code> by the Kubernetes scheduler. After resolving scheduling constraints and startup capacity, it creates a Node and binds the Pod. This stateless approach will help you avoid race conditions and improve performance. If there is a problem with a launched Node, Kubernetes will automatically migrate the Pod to a new Node. When Karpenter launches a Node, it will also allow Kubernetes' scheduler to schedule on it.&lt;/p>
&lt;h2 id="cloud-provider">Cloud provider&lt;/h2>
&lt;p>Karpenter makes a request to the relevant cloud provider for provisioning a new Node. The first supported cloud provider is AWS, but Karpenter is designed to work with other cloud providers as well. While using the well-known labels of Kubernetes, the provisioner can set a number of values specific to the cloud provider.&lt;/p>
&lt;p>If you are developing your own provider, you can create it in the repository under &lt;code>pkg/cloudprovider/&lt;/code>. The directory structure is as follows. The &lt;code>fake&lt;/code> directory is provided as an example for reference.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">.
├── aws
│   ├── apis
│   │   └── v1alpha1
│   └── fake
├── fake
├── metrics
└── registry
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>First, you need to create the following files for each cloud provider under &lt;code>pkg/cloudprovider/registry&lt;/code> to register them.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-golang" data-lang="golang">&lt;span class="c1">// +build &amp;lt;YOUR_PROVIDER_NAME&amp;gt;
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kn">import&lt;/span> &lt;span class="p">(&lt;/span>
&lt;span class="s">&amp;#34;github.com/aws/karpenter/pkg/cloudprovider/&amp;lt;YOUR_PROVIDER_NAME&amp;gt;&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="kd">func&lt;/span> &lt;span class="nf">NewCloudProvider&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="nx">cloudprovider&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">CloudProvider&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nx">YOUR_PROVIDER_NAME&lt;/span>&lt;span class="p">&amp;gt;.&lt;/span>&lt;span class="nf">NewCloudProvider&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>You can also create one for each cloud provider under &lt;code>pkg/cloudprovider&lt;/code> for your environment. If you check the &lt;code>fake&lt;/code> directory, you will find the following files. You can add other necessary information according to your environment.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">.
├── cloudprovider.go
└── instancetype.go
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="difference-from-cluster-auto-scaler">Difference from Cluster Auto scaler&lt;/h2>
&lt;p>Like Karpenter, the Kubernetes Cluster Auto scaler is designed to add Nodes when a request comes in to run a Pod that cannot be handled by the current capacity.
Cluster Auto scaler is part of the Kubernetes project and is implemented by most of the major Kubernetes cloud providers. By rethinking provisioning, Karpenter provides the following improvements.&lt;/p>
&lt;h3 id="designed-to-take-advantage-of-the-flexibility-of-the-cloud">Designed to take advantage of the flexibility of the cloud&lt;/h3>
&lt;p>Karpenter has the ability to efficiently handle any type of instance available in AWS. Cluster Autoscaler was not originally built with the flexibility to support hundreds of instance types, zones, and purchase options.&lt;/p>
&lt;h3 id="group-less-node-provisioning">Group less Node Provisioning&lt;/h3>
&lt;p>Karpenter manages each instance directly, without orchestration mechanisms such as Node groups. This allows you to retry in milliseconds instead of minutes if capacity is not available. It also allows you to take advantage of a variety of instance types, availability zones, and purchasing options without having to create hundreds of Node groups.&lt;/p>
&lt;h3 id="scheduling-implementation">Scheduling Implementation&lt;/h3>
&lt;p>Cluster Autoscaler does not bind a Pod to the Node it creates. Instead, it relies on the &lt;code>kube-scheduler&lt;/code> to make the same scheduling decision after the Node comes online. The Node started by Karpenter is bound to its Pod immediately. kubelet&lt;code> does not need to wait for the scheduler or Node to be ready. kubelet&lt;/code> does not need to wait for the scheduler or Node to be ready, it can start preparing the container runtime immediately, including pre-pulling images. This can reduce the Node startup latency by a few seconds.&lt;/p>
&lt;h2 id="thoughts">Thoughts&lt;/h2>
&lt;p>In this article, I&amp;rsquo;ve tried to dig a little deeper into Karpenter.&lt;/p>
&lt;p>Personally, I think it&amp;rsquo;s the same as GKE Autopilot&amp;rsquo;s dynamic Node provisioning process. I think Karpenter is an OSS version of that tool. Like GKE Autopilot, Karpenter observes the specification of non-schedulable pods, computes aggregate resource requests, and sends the requests to an underlying compute service (such as Amazon EC2) that has the capacity required to run all the pods.&lt;/p>
&lt;p>Karpenter also allows you to define custom resources and specify the provisioning configuration for the following Nodes. We found the flexibility to change the configuration to be a significant advantage.&lt;/p>
&lt;ul>
&lt;li>Instance size/type, topology (zone, etc.)&lt;/li>
&lt;li>Architecture (arm64, amd64, etc.)&lt;/li>
&lt;li>Lifecycle type (spot, on-demand, pre-emptive, etc.)&lt;/li>
&lt;/ul>
&lt;p>On the other hand, Karpenter can also deprovision a Node when it is no longer needed. This can be determined by setting the Node&amp;rsquo;s expiration date (&lt;code>ttlSecondsUntilExpired&lt;/code>) or when the last workload running on a Karpenter provisioned Node has finished (&lt;code>ttlSecondsAfterEmpty&lt;/code>). Either of these two events triggers a finalization that codes the Node, ejects the Pod, terminates the underlying compute resource, and deletes the Node object. This deprovisioning feature can also be used to keep Node up-to-date with the latest AMI.&lt;/p>
&lt;p>With Karpenter, I believe you can offload Node provisioning, autoscaling and upgrading and focus on running your application. Karpenter works with all kinds of Kubernetes applications, but I think it performs especially well in use cases where large amounts of diverse compute resources need to be provisioned and de-provisioned quickly. (Training machine learning models, running simulations, batch jobs with complex financial calculations, etc.)&lt;/p>
&lt;p>Currently, it only runs on AWS, but we will keep an eye on it in the future. If I have time, I&amp;rsquo;ll try to implement it on other clouds as well.&lt;/p></description></item><item><title>Karpenter を調べてみた</title><link>https://example.com/p/karpenter-%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://example.com/p/karpenter-%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/</guid><description>&lt;img src="https://example.com/p/karpenter-%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/featured-image.webp" alt="Featured image of post Karpenter を調べてみた" />&lt;p>先日、AWS re:Invent にて Kubernetes クラスターで Node の自動スケーリングをする Karpenter が GA になりました.&lt;/p>
&lt;p>今回は、それについて深堀りしてみます.&lt;/p>
&lt;h2 id="はじめに">はじめに&lt;/h2>
&lt;p>これは &lt;a class="link" href="https://qiita.com/advent-calendar/2021/kubernetes" target="_blank" rel="noopener"
>Kubernetes Advent Calendar 2021&lt;/a> 18 日目の記事です.&lt;/p>
&lt;h2 id="karpenter-とは">Karpenter とは&lt;/h2>
&lt;p>Karpenter は、&lt;strong>「Just-in-time Nodes for Any Kubernetes Cluster」&lt;/strong> と公式で記載されている通り、スケジュールが不能な Pod に対して、瞬時に新しい Node をプロビジョニングする機能を提供します. それにより、Kubernetes クラスター上でワークロードを実行する際の効率とコスト改善をゴールとしています.&lt;/p>
&lt;p>Karpenter の動作は以下の通りになります.&lt;/p>
&lt;ul>
&lt;li>Kubernetes スケジューラがスケジューリング不能とマークした Pod を監視する&lt;/li>
&lt;li>Pod から要求された以下のスケジューリング制約の評価
&lt;ul>
&lt;li>リソース要求&lt;/li>
&lt;li>Node セレクタ&lt;/li>
&lt;li>アフィニティ&lt;/li>
&lt;li>トレラント&lt;/li>
&lt;li>トポロジー拡散制約&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Pod の要件を満たす Node のプロビジョニング&lt;/li>
&lt;li>新しい Node で実行する Pod のスケジューリング&lt;/li>
&lt;li>Node が不要になったら削除する&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Karpenter の利用方法について&lt;/p>
&lt;p>Karpenter は 2021 年 12 月 現在、AWS のみをサポートしています.&lt;/p>
&lt;/blockquote>
&lt;h2 id="kubernetes-におけるオートスケール">Kubernetes におけるオートスケール&lt;/h2>
&lt;p>Kubernetes には、Pod と Node それぞれにオートスケールする機能が提供されています.&lt;/p>
&lt;h3 id="pod">Pod&lt;/h3>
&lt;p>Pod には、以下の 2 種類のスケール方法があります.&lt;/p>
&lt;h4 id="水平スケール-horizontal-pod-autoscaler">水平スケール (Horizontal Pod Autoscaler)&lt;/h4>
&lt;p>Pod の水平スケールは、Pod 数を増やすことにより処理性能を向上させるスケール方法です. CPU やメモリなど、ユーザが独自に設定したメトリクスなども判断の材料として使えます.&lt;/p>
&lt;p>Pod 数は以下の計算式で算出されます.&lt;/p>
&lt;p>&lt;code>希望するレプリカ数 = ceil[&amp;lt;現在の Pod数&amp;gt; * (&amp;lt;現在の指標値 / &amp;lt;ターゲットとする指標値&amp;gt;)]&lt;/code>&lt;/p>
&lt;h4 id="垂直スケール-vertical-pod-autoscaler">垂直スケール (Vertical Pod Autoscaler)&lt;/h4>
&lt;p>Pod が利用可能なリソースを増やすことで処理性能を向上させるスケール方法です. こちらは、CPU やメモリを判断材料に使用します. どちらかと言うとリソース使用率の最適化を行っているイメージです.&lt;/p>
&lt;h3 id="node">Node&lt;/h3>
&lt;h4 id="node-の水平オートスケーラー-cluster-autoscaler">Node の水平オートスケーラー (Cluster Autoscaler)&lt;/h4>
&lt;p>ワーカー Node の台数を増やすことによって処理性能を向上させるスケール方法です. Pod の水平スケールなどと連携することも可能です.&lt;/p>
&lt;h2 id="インストール方法">インストール方法&lt;/h2>
&lt;p>Karpenter は、Helm Chart でクラスタにインストールされます.
Karpenter はさらに、IAM Roles for Service Accounts (IRSA)を必要とします.&lt;/p>
&lt;p>現在、Karpenter を使用する際に必要なユーティリティは以下の通りです.&lt;/p>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html" target="_blank" rel="noopener"
>AWS CLI&lt;/a>&lt;/li>
&lt;li>&lt;code>kubectl&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank" rel="noopener"
>the Kubernetes CLI&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>eksctl&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html" target="_blank" rel="noopener"
>the CLI for AWS EKS&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>helm&lt;/code>
&lt;ul>
&lt;li>&lt;a class="link" href="https://karpenter.sh/docs/getting-started/#install" target="_blank" rel="noopener"
>the package manager for Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>AWS への Karpenter のインストール方法は、&lt;a class="link" href="https://karpenter.sh/docs/getting-started/#install" target="_blank" rel="noopener"
>こちら&lt;/a>の公式ドキュメントの &lt;strong>「Getting Started with Karpenter on AWS」&lt;/strong> を参考にすると良いと思います.&lt;/p>
&lt;p>Karpenter の Helm Chart は&lt;a class="link" href="https://github.com/aws/karpenter/tree/main/charts/karpenter" target="_blank" rel="noopener"
>こちら&lt;/a>から確認することができます.&lt;/p>
&lt;blockquote>
&lt;p>Terraform を用いたインストール&lt;/p>
&lt;p>&lt;a class="link" href="https://learn.hashicorp.com/tutorials/terraform/install-cli" target="_blank" rel="noopener"
>Terraform&lt;/a> を使用したインストール方法もあります.詳しくは&lt;a class="link" href="https://karpenter.sh/docs/getting-started-with-terraform/" target="_blank" rel="noopener"
>こちら&lt;/a>を参照してください.&lt;/p>
&lt;/blockquote>
&lt;p>概要図は下図の通りです。&lt;/p>
&lt;p>&lt;img src="https://example.com/karpenter-overview.webp"
loading="lazy"
alt="Karpenter Overview"
>&lt;/p>
&lt;h2 id="プロビジョナーの設定">プロビジョナーの設定&lt;/h2>
&lt;p>Karpenter の仕事は、スケジュールできない Pod を処理する Node を追加し、その Node で Pod をスケジュールし、不要になったら Node を削除することです.&lt;/p>
&lt;p>Karpenter を設定するには、Karpenter がスケジューリング不能な Pod と期限付き Node を管理する方法を定義するプロビジョナーを作成します.&lt;/p>
&lt;p>以下は、Karpenter のプロビジョナーについて知っておくと良いと思います.&lt;/p>
&lt;h3 id="unschedulable-pods">Unschedulable pods&lt;/h3>
&lt;p>Karpenter は、ステータス条件 &lt;code>Unschedulable=True&lt;/code> を持つ Pod のみをプロビジョニングしようとします. これは、kube-scheduler が既存の容量に Pod をスケジュールすることに失敗したときに設定されます.&lt;/p>
&lt;h3 id="provisioner-cr">Provisioner CR&lt;/h3>
&lt;p>Karpenter では、プロビジョニング構成を指定するために、&lt;code>Provisioner&lt;/code> というカスタムリソースを定義しています.&lt;/p>
&lt;p>各プロビジョナーは個別の Node セットを管理しますが、Pod はそのスケジューリング制約をサポートする任意のプロビジョナーにスケジュールすることができます.&lt;/p>
&lt;p>プロビジョナーには、プロビジョニング可能な Node と Node の属性（Node を削除するためのタイマーなど）に影響を与える制約が含まれています.&lt;/p>
&lt;p>以下が プロビジョナーのリソースになります.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">karpenter.sh/v1alpha5&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Provisioner&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">default&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ttlSecondsUntilExpired&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2592000&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">ttlSecondsAfterEmpty&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">30&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">taints&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">example.com/special-taint&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">effect&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">NoSchedule&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">billing-team&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">my-team&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requirements&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;node.kubernetes.io/instance-type&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;m5.large&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;m5.2xlarge&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;us-west-2a&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;us-west-2b&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;kubernetes.io/arch&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;arm64&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;amd64&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;karpenter.sh/capacity-type&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;spot&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;on-demand&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">provider&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>{}&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>spec.ttlSecondsUntilExpired&lt;/code>は、コントローラが Node を終了するまでに待つ秒数で、Node の作成時から計測されます. これは、最終的に一貫した Node アップグレード、メモリリーク保護、破壊テストのような機能を実装するのに便利です. このフィールドが設定されていない場合、有効期限切れによる終了は無効になります.&lt;/p>
&lt;p>&lt;code>spec.ttlSecondsAfterEmpty&lt;/code>は、Node が空になったことを検出した時点から、コントローラが Node を削除しようとするまでに待つ秒数です. Node は、デーモンセットを除いて、その Node にスケジュールされている Pod がない場合、空であると見なされます.&lt;/p>
&lt;p>&lt;code>spec.requirements&lt;/code>は、プロビジョニングされた Node のパラメータを制約します. &lt;code>nodeAffinity&lt;/code> や &lt;code>nodeSelector&lt;/code> と組み合わせることも可能です. 演算子 &lt;code>{ In, NotIn }&lt;/code> は、値を含めたり除外したりするためにサポートされています.&lt;/p>
&lt;h2 id="node-のデプロビジョニング">Node のデプロビジョニング&lt;/h2>
&lt;p>Karpenter では、不要になった Node を以下のように削除しています.&lt;/p>
&lt;h3 id="finalizer">Finalizer&lt;/h3>
&lt;p>Karpenter は、作成する各 Node にファイナライザービットを配置します。&lt;/p>
&lt;p>これらの Node を削除するリクエスト（TTL や手動での kubectl による Node 削除など）が来ると、Karpenter は Node をコード化し、すべての Pod を排出して EC2 インスタンスを終了させ、Node オブジェクトを削除する.&lt;/p>
&lt;p>Karpenter は、Node を適切に削除するために必要なすべてのクリーンアップ作業を処理します.&lt;/p>
&lt;h3 id="node-expiry">Node Expiry&lt;/h3>
&lt;p>Node の有効期限値 (&lt;code>ttlSecondsUntilExpired&lt;/code>)に達すると、その Node は（まだワークロードを実行していても）Pod から排出され、削除されます.&lt;/p>
&lt;h3 id="empty-nodes">Empty Nodes&lt;/h3>
&lt;p>Karpenter が管理する Node で稼働している最後のワークロード Pod がなくなると、その Node には &lt;code>emptiness&lt;/code> タイムスタンプが付与されます。その「Node が空になる」有効期限 (&lt;code>ttlSecondsAfterEmpty&lt;/code>) 達すると、ファイナライズがトリガーされます.&lt;/p>
&lt;blockquote>
&lt;p>Node を削除する方法について&lt;/p>
&lt;p>Karpenter が Node を削除する方法の詳細については、Node のデプロビジョニングの&lt;a class="link" href="https://karpenter.sh/docs/tasks/deprov-nodes/" target="_blank" rel="noopener"
>詳細&lt;/a>を参照してください.&lt;/p>
&lt;/blockquote>
&lt;h2 id="node-のアップグレード">Node のアップグレード&lt;/h2>
&lt;p>Node をアップグレードする簡単な方法は、&lt;code>ttlSecondsUntilExpired&lt;/code> を設定することです。Node は設定された期間後に終了し、より新しい Node と入れ替わります.&lt;/p>
&lt;h2 id="制約条件">制約条件&lt;/h2>
&lt;p>プロビジョナーで定義された制約や、デプロイされる Pod から要求された制約がないため、Karpenter はクラウドプロバイダが利用できる機能全体から選択されます. Node は、任意のインスタンスタイプを使用して作成し、任意のゾーンで実行することができます.&lt;/p>
&lt;h2 id="スケジューリング">スケジューリング&lt;/h2>
&lt;p>Karpenter は、Kubernetes のスケジューラーが &lt;code>unschedulable&lt;/code> とマークした Pod をスケジュールします. スケジューリング制約と起動容量を解決した後、Node を作成し、Pod をバインドします. このステートレスなアプローチは、レースコンディションを回避し、パフォーマンスを向上させるのに役立ちます. 起動した Node に何か問題があれば、Kubernetes は自動的に新しい Node に Pod を移行します.Karpenter が Node を立ち上げると、その Node は Kubernetes のスケジューラーがその上でをスケジュールすることも可能になります.&lt;/p>
&lt;h2 id="クラウドプロバイダー">クラウドプロバイダー&lt;/h2>
&lt;p>Karpenter は、関連するクラウドプロバイダーに新しい Node のプロビジョニングの要求を行います. 最初にサポートされるクラウドプロバイダーは AWS ですが、Karpenter は他のクラウドプロバイダーでも動作するように設計されています. Kubernetes のよく知られたラベルを使用しながら、プロビジョナーは、クラウドプロバイダーに固有のいくつかの値を設定することができます.&lt;/p>
&lt;p>個人でプロバイダーの開発する場合は、リポジトリの&lt;code>pkg/cloudprovider/&lt;/code>配下に作成します. ディレクトリ構造は以下の通りです. &lt;code>fake&lt;/code>ディレクトリは、参考例として用意されています.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">.
├── aws
│   ├── apis
│   │   └── v1alpha1
│   └── fake
├── fake
├── metrics
└── registry
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>まず、&lt;code>pkg/cloudprovider/registry&lt;/code>配下で、クラウドプロバイダー毎に以下の以下のファイルを作成することで登録ができます.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-golang" data-lang="golang">&lt;span class="c1">// +build &amp;lt;YOUR_PROVIDER_NAME&amp;gt;
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kn">import&lt;/span> &lt;span class="p">(&lt;/span>
&lt;span class="s">&amp;#34;github.com/aws/karpenter/pkg/cloudprovider/&amp;lt;YOUR_PROVIDER_NAME&amp;gt;&amp;#34;&lt;/span>
&lt;span class="p">)&lt;/span>
&lt;span class="kd">func&lt;/span> &lt;span class="nf">NewCloudProvider&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="nx">cloudprovider&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">CloudProvider&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nx">YOUR_PROVIDER_NAME&lt;/span>&lt;span class="p">&amp;gt;.&lt;/span>&lt;span class="nf">NewCloudProvider&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>また、&lt;code>pkg/cloudprovider&lt;/code>配下で、クラウドプロバイダーごとに環境に合わせて作成します. &lt;code>fake&lt;/code> ディレクトリを確認すると以下のファイルが用意されています. その他の必要な情報は環境に合わせて追加すると良いです.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">.
├── cloudprovider.go
└── instancetype.go
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="cluster-autoscaler-との違い">Cluster Autoscaler との違い&lt;/h2>
&lt;p>Karpenter と同様に、Kubernetes Cluster Autoscaler は、現在のキャパシティでは対応できない Pod の実行要求が来たときに、Node を追加するように設計されています.
Cluster Autoscaler は Kubernetes プロジェクトの一部であり、ほとんどの主要な Kubernetes クラウドプロバイダーが実装しています. プロビジョニングを見直すことで、Karpenter は以下の改善を提供しています.&lt;/p>
&lt;h3 id="クラウドの柔軟性を活かした設計">クラウドの柔軟性を活かした設計&lt;/h3>
&lt;p>Karpenter は、AWS で利用できるあらゆる種類のインスタンスに効率的に対応できる能力を備えています. Cluster Autoscaler は、もともと何百ものインスタンスタイプ、ゾーン、購入オプションに対応できるような柔軟性を持って構築されたものではありません.&lt;/p>
&lt;h3 id="グループレスの-node-プロビジョニング">グループレスの Node プロビジョニング&lt;/h3>
&lt;p>Karpenter は、Node グループのようなオーケストレーションの仕組みを使わずに、各インスタンスを直接管理します. これにより、キャパシティが利用できない場合、数分ではなくミリ秒単位で再試行することができます. また、何百もの Node グループを作成することなく、多様なインスタンスタイプ、アベイラビリティゾーン、および購入オプションを活用することができます.&lt;/p>
&lt;h3 id="スケジューリングの実施">スケジューリングの実施&lt;/h3>
&lt;p>Cluster Autoscaler は、作成した Node に Pod をバインドしません. その代わり、Node がオンラインになった後に同じスケジューリング決定を行うために &lt;code>kube-scheduler&lt;/code> に依存します.Karpenter が起動した Node には、すぐにその Pod がバインドされます。kubelet` はスケジューラーや Node の準備が整うのを待つ必要がありません. イメージの事前プルも含め、コンテナランタイムの準備をすぐに開始できます.これにより、Node の起動レイテンシを数秒短縮することができます.&lt;/p>
&lt;h2 id="所感">所感&lt;/h2>
&lt;p>今回は、Karpenter について少し深堀りしてみました.&lt;/p>
&lt;p>個人的には、GKE Autopilot の動的 Node プロビジョニングプロセスと同じなのかなと思っています. Karpenter はそのツールの OSS 版と言えると思います. GKE Autopilot と同様に、Karpenter はスケジューリング不能な Pod の仕様を観測し、集約されたリソース要求を計算し、すべての Pod の実行に必要な容量を持つ基礎的な計算サービス（Amazon EC2 など）に要求を送信します.&lt;/p>
&lt;p>また、Karpenter では、カスタムリソースを定義して、以下の Node のプロビジョニング構成を指定することができます. 構成を柔軟に変更できる点は、かなり大きいメリットだと感じました.&lt;/p>
&lt;ul>
&lt;li>インスタンスサイズ/タイプ、トポロジー(ゾーンなど)&lt;/li>
&lt;li>アーキテクチャ(arm64、amd64 など)&lt;/li>
&lt;li>ライフサイクルタイプ(スポット、オンデマンド、プリエンプティブなど)&lt;/li>
&lt;/ul>
&lt;p>一方、Karpenter は、Node が不要になった場合、デプロビジョンを行うこともできます. これは、Node の有効期限設定 (&lt;code>ttlSecondsUntilExpired&lt;/code>) または Karpenter プロビジョニングされた Node 上で実行されている最後のワークロードが終了したとき (&lt;code>ttlSecondsAfterEmpty&lt;/code>) に決定することができます. この 2 つのイベントのどちらかがトリガーとなり、Node をコード化し、Pod を排出し、基盤となるコンピュートリソースを終了させ、Node オブジェクトを削除するファイナライゼーションが行われます. このデプロビジョニング機能は、Node を最新の AMI で最新の状態に保つためにも使用できます.&lt;/p>
&lt;p>Karpenter を使えば、Node のプロビジョニング、オートスケール、アップグレードをオフロードして、アプリケーションの実行に集中することができると思います. Karpenter はあらゆる種類の Kubernetes アプリケーションで動作しますが、特に、大量の多様な計算リソースを迅速にプロビジョニングおよびデプロビジョニングする必要があるユースケースで優れたパフォーマンスを発揮すると思います. (機械学習モデルのトレーニング、シミュレーションの実行、複雑な金融計算を行うバッチジョブなど)&lt;/p>
&lt;p>現在は、AWS のみでしか動作しませんが今後の動向には注目していきたいと思います. また、時間があれば他クラウドへの実装などもしてみようと思います.&lt;/p></description></item></channel></rss>