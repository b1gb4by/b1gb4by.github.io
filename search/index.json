[{"content":"Kubernetes を初めて学ぶ方に向けて、ロードマップを作成してみました.\nこれから学んでいく第一歩として、参考にしていただければと思います.\nはじめに 本ページは、筆者が全くの知見のない初学者が Kubernetes を学習し、利用していく上で必要な知識やスキルの習得方法を示したものです.\nそのため、本ページでは概要や用語の説明などは行いません.\nまた、本ページの指針を全て達成することで Kubernetes の全てを網羅し、完全に理解できる訳ではありません.\n必要な知識が必要な場合は個人で別途で調査、理解をする必要があることを予めご了承ください.\nそもそも Kubernetes とは？ Kubernetes の読み方 Kubernetes は、クバネティス、クバネテス、クーべネティスと呼び方は様々ありますが、読み方は人それぞれです.\nまた、K8sと略されることもあります.\n \u0026ldquo;Kubernetes\u0026rdquo;(希: κυβερνήτης, koo-ber-nay'-tace, クベルネテス)は、ギリシャ語で航海長または水先案内人を意味し、サイバネティクス(人工頭脳学)の語源でもある\n引用: https://ja.wikipedia.org/wiki/Kubernetes\n 何をしているの？ Kubernetes は、コンテナをデプロイできる基盤となるソフトウェアであり、コンテナオーケストレーターの 1 つです.\nもともとは Google のコンテナ盤である Borg をベースにした OSS であり、2014 年 6 月にローンチされました.\n現在は Cloud Native Computing Foundation (CNCF) によって管理されている GA Project の 1 つです.\n CNCF とは？\nCNCF は、Cloud Native なシステムを推進するために 2015 年に創設された Linux Foundation 傘下の非営利団体の 1 つです.\n 学習ロードマップ 本題の学習のロードマップについてですが、個人的には以下の通りに学習を進めて行くのが良いと考えています.\nStep1. コンテナの基礎理解 Kubernetes を理解する上で、非常に密な関係性にあるのがコンテナです.\nまずは、コンテナについて理解を深めることが最重要になります. コンテナを理解している場合は、このステップはスキップしても良いでしょう.\n理解がないまま Kubernetes を理解を深めること困難です. まずは、コンテナの理解を深めしょう.\nコンテナと言っても、コンテナを動かすエンジンは複数種類あるので、まずはDockerに焦点をあてて、学習を進めて行きます.\nStep1 を学ぶポイント  コンテナとは何なのか？ コンテナと仮想化の違いは？ Dockerfile の記述方法 Docker の操作方法  イメージのビルド イメージのプッシュ コンテナの起動    Step2. Kubernetes の基礎理解 コンテナの基礎を理解した後は、Kubernetes の基礎を理解していきます.\nKubernetes を動かす前にまずは知識としてインプットしていくのが良いでしょう.\nStep2 を学ぶポイント  Kubernetes で実現できること  また、それがなぜ必要とされているのか Kubernetes を使うメリット   API リソース群の理解 kubectl の使用方法  Step3. Kubernetes の利用 コンテナや Kubernetes の基礎的な部分を理解した後は、実際に Kubernetes を触ってみましょう.\n実際に Kubernetes を動かして、体系的に学ぶようにすることで理解を深めていきます.\nStep1, Step2 でインプットした知識を、きちんとアウトプットしていくことでラーニングサイクルを回すようにしましょう.\nStep3 を学ぶポイント  Kubernetes の一連の動作を試してみる  クラスターの作成 アプリケーションの操作 (kubectlでの操作)  デプロイ 探索 公開 スケーリング アップデート      Step4. エコシステムの利用 Kubernetes には多くのエコシステムが提供されています.\nエコシステムを活用することで、Kubernetes はより真価を発揮します.\nまた、本番環境などで運用する際、CI/CD や監視、ログ収集、セキュリティなど考慮する点が多く存在します.\nエコシステムを利用することで、それらの課題を解決することができます.\n エコシステムとは？\n元々は生態系の用語であり、あるエリア(地域や空間など)の生命体が互いに依存しながら生態を維持する関係の様子を表します. ここでは、Kubernetes を支えるツール群のことを指しています.\n Step4 を学ぶポイント  Kubernetes を運用する上でどのような点を考慮する必要があるのか どのようなエコシステムがあるのか  Step5. 内部ツールの理解・実装 しかし、実際にマネージドクラウド(AWS, GCP, Azure など)で運用する場合、あまり気にする必要はありません.\n個人で Kubernetes のコントローラーなどを開発していきたい方は Step 5 に挑戦してみましょう.\nなお、Kubernetes 関連の周辺ツールは、Go 言語で書かれている事が多いです.\nGo 言語を初めて触る方は、後日記載予定の「Go 言語学習ロードマップ」をご覧いただければと思います.\nStep5 を学ぶポイント  カスタムコントローラーの基礎の理解 公開されているカスタムコントローラー群の理解 カスタムコントローラーの実装、動作検証  おすすめの書籍、文献など ここでは、ロードマップにそって学習を行う上で著者自身がお薦めする書籍や、文献などを紹介します.\n書籍 仕組みと使い方がわかる Docker＆Kubernetes のきほんのきほん  対象  Step1 Step2   コンテナに初めて触れる方やバックエンドの技術に詳しくない方でも Linux の知識や、サーバの基礎なども併せて記載されているため読みやすいです イラストも多く、視覚的に知識をインプットすることができます  Docker コンテナ開発・環境構築の基本  対象  Step1 Step2   コンテナを体系的に学ぶことができます コンテナを用いた CI/CD、アプリケーションの運用方法も基本から記載されています  イラストでわかる Docker と Kubernetes  対象  Step1 Step2   Docker や Kubernetes の知識を有していない場合はまずこれを読んだほうがイメージはつかみやすいかと思います  概要がメインなので、サラッと読むには良いと思います    Kubernetes 完全ガイド 第 2 版  対象  Step2 Step3   一番時初めにこの書籍から読み始めるのは少し難易度が高いため、上記の書籍を読んでから読むと良いです Kubernetes に関する知識が網羅的にまとめられています Kubernetes を使用していて、困った時にリファレンスとしても活用できます  Kubernetes CI/CD パイプラインの実装  対象  Step4   題名の通り、CI/CD のパイプラインを構築について詳細に記されている  Docker/Kubernetes 開発・運用のためのセキュリティ実践ガイド  対象  Step3 Step4   Docker や Kubernetes の知識はある程度有していないと読むのは難しいと思います Kubernetes 上での開発、運用視点からのセキュリティ対策についてまとめられていています  実践入門 Kubernetes カスタムコントローラーへの道  対象  Step5   Kubernetes で拡張機能(カスタムコントローラー)を開発する入門です  Kubernetes ネットワーク 徹底解説  Kubernetes のネットワークの実現方法について説明されています  \ne-Learning Katacoda Katakodaは、ブラウザ環境で学習が行えるプラットフォームです.\nコンテナ周辺の技術などのトレーニングが公開されており、全てブラウザ環境でコマンド、ターミナル環境が提供されているため、環境構築などの手間などなく学習を行うことができます.\nKubernetes 学習とトレーニング (Microsoft) Microsoft が提供している、トレーニングになります.\n動画ベースで各項目を説明しており、Kubernetes の基本を理解すると共に、様々な Kubernetes の機能などについても解説してくれています.\nトレーニングは、こちらから受講することができます.\nつくって学ぶ Kubebuilder @zoetropeさんによって提供されており、Kubebuilder と呼ばれるフレームワークを使用して、Kubernetes を拡張するためのカスタムコントローラー/オペレーターを開発するため方法を解説されています.\nStep5 に進まれた方は、この記事を読んでカスタムコントローラー/オペレーターの開発手法を理解すると良いでしょう.\nその他 情報収集 Kubernetes の情報を収集する際には基本的には以下を使用しています.\n Kubernetes Official  Document Reference Blog   GitHub  kubernetes/kubernetes   News  KubeWeekly Kubernetes Podcast Kubernetes on Medium    コミュニティの参加 Kubernetes Slack Kubernetes の Slack になります. Join はこちらからできます.\n※ 既に Join 済みの場合は、こちらからサインインできます.\n初めて参加する方は以下のチャンネルに参加すると良いと思います.\n #jp-users jp-events jp-dev kubernetes-doc-ja jp-mentoring jp-users-novice  初学者の方で質問などを行いたい場合は、jp-users-noviceで聞いてみると良いでしょう.\n勉強会など Kubernetes Meetup Tokyo 国内では最大の Kubernetes の勉強会になります.\n過去の勉強会の動画はこちらから見ることができます.\nKubernetes Meetup Novice 「まだ、学び始めたばっかりだけど発表してみたい」という方向けの勉強会になります.\n過去の勉強会の動画はこちらから見ることができます.\nKubernetes 変更内容共有会 Kubernetes のバージョンアップごとの変更内容で重要な部分や、面白いポイントを紹介してくれる勉強会になります.\nKubernetes のバージョンに追従してくためにこちらの勉強会を活用してみても良いと思います.\n過去の勉強会の動画はこちらから見ることができます.\nKubernetes Internal Kubernetes をより深く掘り下げ、その他の周辺ツールなどの実装や設計などについて情報交換、交流をするための勉強会です.\n現在は、「kubenews」が毎週開催されています.\n過去の勉強会の動画はこちらから見ることができます.\nOCHa(Oracle Cloud Hangout cafe) Cafe Kubernetes 以外にクラウドネイティブな技術全般について発信しています.\n過去の勉強会の動画はこちらから見ることができます.\nKubeConn + CloudNativeConn KubeCon + CloudNativeCon は、Cloud Native Computing Foundation (CNCF) の主力カンファレンスカンファレンスとして開催されています.\nKubernetes のみならず、クラウドネイティブに関するコミュニティが一同に集結し、クラウドネイティブな技術の教育と進歩を推進しています.\nCloud Native Days 日本最大級のクラウドネイティブ・テックカンファレンスを開催しています.\n過去のカンファレンスの動画はこちらから見ることができます.\n資格 Kubernetes では、Linux Foundation と Cloud Native Computing Foundation (CNCF) が行っている試験が一番有名だと思います.\nCKA (Certified Kubernetes Administrator) Kubernetes の基本的な機能に加え、基盤自体の運用方法やトラブルシュートなどの理解が求められます.\nKubernetes の基盤開発者向けの試験になります.\n※ 日本語での試験が可能です.\nCKAD (Certified Kubernetes Application Developer) CKA と比べ、アプリケーション開発者向けの試験になります.\nKubernetes の基本的な機能に加え、Kubernetes をアプリをデプロイ・運用する基盤として十分に利用できるスキルがあるかが求められます.\n※ 日本語での試験が可能です.\nCKS (Certified Kubernetes Security Specialist) CKA の試験を更にセキュリティに焦点を当てた試験となります.\nこの試験を受けるには、事前に CKA に合格している必要があります.\nまとめ 今回は、Kubernetes の学習ロードマップを考えてまとめてみました.\n筆者自身も Kubernetes を触り始めてまだ日が浅いため、上記のものを参考に日々精進しています.\nこれをきっかけに一人でも多くの方が Kubernetes に興味をもっていただけることを切に願います.\n","date":"2022-05-13T00:00:00Z","image":"https://example.com/p/kubernetes%E5%88%9D%E5%AD%A6%E8%80%85%E3%83%AD%E3%83%BC%E3%83%89%E3%83%9E%E3%83%83%E3%83%97/k8s-beginner-loadmap_huf01b21201a300d988eff3892eccc213a_261362_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/kubernetes%E5%88%9D%E5%AD%A6%E8%80%85%E3%83%AD%E3%83%BC%E3%83%89%E3%83%9E%E3%83%83%E3%83%97/","title":"Kubernetes初学者ロードマップ"},{"content":"1 月 26 日に、Rancher Desktop が正式に v1.0.0 としてリリースされました.\n今回は、実際に Rancher Desktop を MXLinux にインストールして動かそうと思います.\nRancher Desktop とは Rancher Desktop は Electron と Node.js をベースに構築されたデスクトップアプリケーションであり、デスクトップ上で Kubernetes とコンテナ管理を実行することができます.\nまた、実行する Kubernetes のバージョンを自由に選択することができます.\ncontainerd、またはMoby (dockerd)を使って、コンテナイメージのビルド、プッシュ、プル、実行が可能です. ビルドしたコンテナイメージは、レジストリを必要とせず、すぐに Kubernetes で実行できます.\n必要条件 OSS のデスクトップアプリケーションで、macOS や Windows、そして Linux の様々な環境で動作させることができます. M1 にも対応しているため、ほとんどの環境で動作が可能になりました.\n環境での必要条件は以下の通りです.\nmacOS  macOS  Catalina 10.15 以降   CPU アーキテクチャ  Apple Silicon（M1）または VT-x 搭載の Intel CPU    WindowsOS  Windows  Windows 10 ビルド 1909 以降 Home エディションにも対応   Hyper-V(仮想化機能) が有効化されている Windows Subsystem for Linux (WSL)  Rancher Desktop は、Windows 上で WSL が必要ですが、セットアップの一部として自動的にインストールされます 手動でディストリビューションをダウンロードは不要    Linux  .debや.rpmパッケージ、またはAppImagesをインストールできるディストリビューション  マシンスペック  8GB のメモリー 4 コアの CPU  動作の仕組み Rancher Desktop は、他のツールをラッピングしながら、動作を実現しています.\nMacOS と Linux では、Lima や QEMU といった仮想マシンを活用してcontainerdまたはdockerdと Kubernetes (k3s) を実行します.\nWindows システムでは、Windows Subsystem for Linux v2 (WSL2) を利用しています.\n 下図は rancher から引用  Rancher Desktop では、NERDCTL プロジェクトと Docker CLI を利用して、イメージを build、push、pull する機能が用意されています. なお、nerdctl と docker の両方が自動的にパスに入れられます. Windows ではインストーラー中に、macOS と Linux では初回実行時に行われます.\nいずれのツールを使用する場合も、Rancher Desktop が適切なコンテナランタイムで実行されている必要があります.\nnerdctlの場合は、containerdランタイムを使用します。docker の場合は、dockerd (moby)ランタイムを使用します.\n Lima について\nLima は WSL と似ており、自動ファイル共有とポート転送、および containerd を備えた Linux の仮想マシンを起動します. Lima は macOS ホストで使われることが想定されていますが、Linux ホストでも使うことができます.\n  QEMU について\nQEMU は、OSS の PC エミュレーターです. x86 や SPARC、MIPS といったさまざまな CPU 上の Linux、Windows などで動作し、ほかの CPU の命令をネイティブコードに変換しながら実行できるという特徴を持ちます.\n  k3s について\nk3s は、Rancher Labs 社が発表した軽量な Kubernetes の 1 つで、40MB 未満のバイナリと、わずか 512MB のメモリ使用量を特徴としています. 最近では、IoT や Edge コンピューティングなどでの活用が期待されています.\n  nerdctl について\nnerdctl は containerd 向けのコンテナの操作ツールです. containerd 用の docker コマンドだと考えていただければと思います.\n MXLinux へのインストール Rancher Desktop を Linux ディストリビューションの 1 つである MXLinux にインストールしてみます.\nインストール方法は公式ドキュメントに従って、Rancher Desktop のリポジトリを追加し、Rancher Desktop をインストールします.\nLinux へのインストールはいくつかのパッケージがありますが、MXLinux は Debian(stable) ベースのため、.debパッケージを使用します.\n1 2 3 4 5 6 7 8 9  # リポジトリを取得し、登録 $ curl https://download.opensuse.org/repositories/isv:/Rancher:/stable/deb/Release.key | sudo apt-key add - $ sudo add-apt-repository \u0026#39;deb https://download.opensuse.org/repositories/isv:/Rancher:/stable/deb/ ./\u0026#39; # パッケージ一覧を更新 $ sudo apt update # Rancher Desktop をインストール $ sudo apt install rancher-desktop   以上で、Rancher Desktop のインストールは完了です. 非常に簡単ですね.\n MXLinux について\nMXLinux は DistroWatch.com でも 注目度の高い Linux ディストリビューションです. antiX と旧 MEPIS Linux コミュニティ間の共同事業として構築されたプロジェクトでギリシャおよびアメリカにて開発されています.\n Rancher Desktop の起動 インストールした Rancher Desktop を起動してみます. アプリケーション自体は非常にシンプルでした.\nGeneral Kubernetes Setting 次に、Kubernetes の設定を見てみます.\nKubernetes versionでは、Kubernetes のバージョンを指定できます. 執筆時点での最新バージョンである v1.23.3 から、最も古いバージョンで v1.16.7 まで選択が可能です.\nPortはデフォルトでは、6443が設定されています.\nContainer runtimeでは、containerdまたはdockerd (moby)のどちらかを選択可能です.\nMemory (GB), CPUsは、メモリー、CPU のコア数を指定できます. 赤いラインまで数値を上げると、下図のように警告文が表示されます.\n一度、環境をクリーンアップしたい場合は、Reset Kubernetesを押すことで簡単にリセットすることができます.\nSupporting Utilities Supporting Utilitiesでは、インストールされたツール郡が表示されています. 既にインストールされていた Docker などに対しては、丁寧に注意喚起が書かれていました.\nImages Images では、Rancher Desktop で使用しているイメージが表示されています. イメージ一覧の⋮からScanを選択すると、イメージの脆弱性を Trivy を使用してスキャンしてくれる.\nここで、Image Namespaceに注目してください. containerdでは、Kubernetes と同様にnamespaceという概念が存在しています. そのため、Kubenetes が namespaceを持つことができるように、containerdも同じようにnamespaceを持つことができます. 上図では、namespace:k8s.ioにイメージが存在しているということになります.\n実際にnerdctlコマンドを使用して、namespaceを確認してみます. nerdctl namespace listでnamespaceの一覧を表示します.\n1 2 3 4  $ ./.local/bin/nerdctl namespace list NAME CONTAINERS IMAGES VOLUMES buildkit 0 0 0 k8s.io 22 16 0   実行してみると、k8s.ioにイメージが存在していることが確認できました.\nまた、nerdctl --namespace k8s.io psでnamespace:k8s.ioで、Rancher Desktop で作成した Kubernetes のコンテナ群が確認できます.\n1 2 3 4 5 6 7 8 9 10 11 12 13  $ ./.local/bin/nerdctl --namespace k8s.io ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1f642d94e7d5 docker.io/rancher/klipper-lb:v0.3.4 \u0026#34;entry\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr/lb-port-443 2ca56d5f0874 docker.io/rancher/mirrored-library-traefik:2.5.6 \u0026#34;/entrypoint.sh --gl…\u0026#34; 49 minutes ago Up k8s://kube-system/traefik-6bb96f9bd8-cflf4/traefik 2f1c800451cf docker.io/rancher/mirrored-coredns-coredns:1.8.6 \u0026#34;/coredns -conf /etc…\u0026#34; 49 minutes ago Up k8s://kube-system/coredns-5789895cd-wgxlq/coredns 56bd8fba2fda docker.io/rancher/klipper-lb:v0.3.4 \u0026#34;entry\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr/lb-port-80 5908afd18045 docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/coredns-5789895cd-wgxlq 8c16131e6d1b docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr 95b859fec9ed docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/local-path-provisioner-6c79684f77-plbxh a0656b86ab35 docker.io/rancher/local-path-provisioner:v0.0.21 \u0026#34;local-path-provisio…\u0026#34; 49 minutes ago Up k8s://kube-system/local-path-provisioner-6c79684f77-plbxh/local-path-provisioner a5c31106d6d7 docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/traefik-6bb96f9bd8-cflf4 c9417746b27d docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/metrics-server-7cd5fcb6b7-4cbkd ce64e7b0a242 docker.io/rancher/mirrored-metrics-server:v0.5.2 \u0026#34;/metrics-server --c…\u0026#34; 49 minutes ago Up k8s://kube-system/metrics-server-7cd5fcb6b7-4cbkd/metrics-server   Troubleshooting Troubleshootingでは、ログの有効化、また Rancher Desktop 自体の初期化ができます.\n検証 Rancher Desktop 上で、実際にコンテナなどを動かして見ます.\nnerdctl の使用 nerdctlコマンドを使用して、nginx を起動してみます. namespace は未指定の場合、デフォルトでdefaultに配置されます.\nnginx を起動 1 2 3 4 5 6  $ ./.local/bin/nerdctl run -d -p 9999:80 nginx docker.io/library/nginx:latest: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767: done |++++++++++++++++++++++++++++++++++++++| ... elapsed: 7.2 s total: 54.1 M (7.5 MiB/s) 484e86556e00843200c97b5aa779ba81a9016796e23964e5a0cac27159de444e   コンテナの状態を確認 1 2 3  $ ./.local/bin/nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 484e86556e00 docker.io/library/nginx:latest \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 0.0.0.0:9999-\u0026gt;80/tcp nginx-484e8   namespace を確認 1 2 3 4 5  $ ./.local/bin/nerdctl namespace list NAME CONTAINERS IMAGES VOLUMES buildkit 0 0 0 default 1 1 0 k8s.io 22 16 0   namespace default のプロセスを確認 1 2 3  $ ./.local/bin/nerdctl --namespace default ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 484e86556e00 docker.io/library/nginx:latest \u0026#34;/docker-entrypoint.…\u0026#34; 11 minutes ago Up 0.0.0.0:9999-\u0026gt;80/tcp nginx-484e8   また、Rancher Desktop からもnamespace:defaultに nginx イメージを確認することができます.\n最後に、localhost:9999にアクセスして、nginx の起動を確認してみます.\nHelm の使用 Rancher Desktop では、起動時に Helm もインストールされるので、Helm を使って Kubernetes に Grafana をデプロイしてみます.\nHelm に Grafana のリポジトリを追加 1  $ ./.local/bin/helm repo add grafana https://grafana.github.io/helm-charts   Helm にリポジトリが追加されていることを確認 1 2 3  $ ./.local/bin/helm repo list NAME URL grafana https://grafana.github.io/helm-charts   追加した Grafana リポジトリからチャートを確認 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ ./.local/bin/helm search repo grafana NAME CHART VERSION APP VERSION DESCRIPTION grafana/grafana 6.21.1 8.3.4 The leading tool for querying and visualizing t... grafana/grafana-agent-operator 0.1.5 0.22.0 A Helm chart for Grafana Agent Operator grafana/enterprise-logs 2.0.0 v1.3.0 Grafana Enterprise Logs grafana/enterprise-metrics 1.7.3 v1.6.1 Grafana Enterprise Metrics grafana/fluent-bit 2.3.0 v2.1.0 Uses fluent-bit Loki go plugin for gathering lo... grafana/loki 2.9.1 v2.4.2 Loki: like Prometheus, but for logs. grafana/loki-canary 0.5.1 2.4.1 Helm chart for Grafana Loki Canary grafana/loki-distributed 0.42.0 2.4.2 Helm chart for Grafana Loki in microservices mode grafana/loki-simple-scalable 0.2.0 2.4.2 Helm chart for Grafana Loki in simple, scalable... grafana/loki-stack 2.5.1 v2.1.0 Loki: like Prometheus, but for logs. grafana/promtail 3.10.0 2.4.2 Promtail is an agent which ships the contents o... grafana/tempo 0.13.0 1.3.0 Grafana Tempo Single Binary Mode grafana/tempo-distributed 0.15.0 1.3.0 Grafana Tempo in MicroService mode grafana/tempo-vulture 0.2.0 1.3.0 Grafana Tempo Vulture - A tool to monitor Tempo...   Helm チャートをリリース 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  $ ./.local/bin/kubectl create namespace monitoring $ ./.local/bin/helm install grafana --namespace monitoring grafana/grafana W0128 03:37:33.477723 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.480386 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.538004 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.538201 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: grafana LAST DEPLOYED: Fri Jan 28 03:37:32 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: 1. Get your \u0026#39;admin\u0026#39; user password by running: kubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo 2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.monitoring.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace monitoring -l \u0026#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace monitoring port-forward $POD_NAME 3000 3. Login with the password from step 1 and the username: admin ################################################################################# ###### WARNING: Persistence is disabled!!! You will lose your data when ##### ###### the Grafana pod is terminated. ##### #################################################################################   上記の手順に従って、正常に Grafana にログインすることができました.\n最後に、kubectlコマンドを使って Grafana が立ち上がっていることを確認します.\n1 2 3 4 5 6 7 8 9 10 11 12  $ ./.local/bin/kubectl get all --namespace monitoring NAME READY STATUS RESTARTS AGE pod/grafana-6b9d4f7f86-mwb4q 1/1 Running 0 15m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana ClusterIP 10.43.218.65 \u0026lt;none\u0026gt; 80/TCP 15m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1/1 1 1 15m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-6b9d4f7f86 1 1 1 15m   以上のように、Rancher Desktop を起動するだけでアプリケーションの開発やデプロイが簡単に行うことができました.\n所感 今回は、v1.0.0 がリリースされた Rancher Desktop を触ってみましたが、非常に完成度の高いツールでした. containerd、nerdctlを初めて触る方も Rancher Desktop は良い機会だと思います.\n昨年、Docker Desktop が有料化が大きなニュースとなり、それに伴い、密かに代替案として注目されていた Rancher Desktop ですが、 Docker Desktop から乗り換えたとしても遜色なく利用できるかと思います. 万が一、Docker のランタイムを使用したいと思えば、Rancher Desktop 上から切り替えれば良いので、大して手間は掛かりません.\nまた、Kubernetes のバージョンをスムーズに切り替えられるのも個人的には大きなメリットなのかなと思います.\nこれから、コンテナを利用する手段の 1 つとして、広く普及していく可能性は大いにあり得るので、今後の動向に注目していきたいと思います.\n","date":"2022-01-27T00:00:00Z","image":"https://example.com/p/mxlinux-%E3%81%A7-rancher-desktop-%E3%82%92%E5%8B%95%E3%81%8B%E3%81%99/featured-image_hua255d1196849d3759ae697794b4989fc_2680_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/mxlinux-%E3%81%A7-rancher-desktop-%E3%82%92%E5%8B%95%E3%81%8B%E3%81%99/","title":"MXLinux で Rancher Desktop を動かす"},{"content":"On January 26th, Rancher Desktop was officially released as v1.0.0.\nIn this article, I\u0026rsquo;ll try to install and run Rancher Desktop on MXLinux.\nWhat is Rancher Desktop? Rancher Desktop is a desktop application built on Electron and Node.js that allows you to run Kubernetes and container management on your desktop.\nYou can choose any version of Kubernetes to run.\nYou can use containerd or Moby (dockerd) to build, push, pull and run container images. The built container images can be run immediately in Kubernetes without the need for a registry.\nRequirements It is an OSS desktop application that can run on macOS, Windows, and various Linux environments. It also supports M1, so it can run on almost any environment.\nThe requirements for each environment are as follows.\nmacOS  macOS.  Catalina 10.15 or later   CPU architecture**  Intel CPU with Apple Silicon (M1) or VT-x    WindowsOS  Windows.  Windows 10 build 1909 or later Home Edition is also supported   Hyper-V (virtualization) is enabled Windows Subsystem for Linux (WSL).  Rancher Desktop requires WSL on Windows, but it will be installed automatically as part of the setup No need to download the distribution manually    Linux  Distributions that can install .deb or .rpm packages, or `AppImages  Machine specs  8GB memory 4-core CPU  How it works Rancher Desktop is wrapping other tools to make it work.\nOn MacOS and Linux, it leverages virtual machines such as Lima and QEMU to run containerd or dockerd and Kubernetes (k3s).\nFor Windows systems, we utilize Windows Subsystem for Linux v2 (WSL2).\n The figure below is taken from rancher how-it-works-rancher-desktop](how-it-works-rancher-desktop.webp)\n Rancher Desktop provides functions to build, push, and pull images using NERDCTL project and Docker CLI. Note that both nerdctl and docker are automatically included in the path. On Windows, this is done during the installer, and on macOS and Linux, it is done at first run.\nTo use either of these tools, Rancher Desktop must be running with the appropriate container runtime.\nFor nerdctl, use the containerd runtime; for docker, use the dockerd (moby) runtime.\n About Lima\nLima is similar to WSL, and boots a Linux virtual machine with automatic file sharing and port forwarding, and containerd. Lima is intended to be used on macOS hosts, but can be used on Linux hosts as well.\n  About QEMU\nQEMU is an OSS PC emulator. It runs on Linux, Windows, etc. on various CPUs such as x86, SPARC, MIPS, etc. It has the feature that it can execute instructions of other CPUs while converting them into native code.\n  About k3s\nk3s is one of the lightweight Kubernetes released by Rancher Labs, featuring a binary size of less than 40MB and a memory usage of only 512MB. Recently, it is expected to be used in IoT and Edge computing.\n  About nerdctl\nnerdctl is a container manipulation tool for containerd. You can think of it as a docker command for containerd.\n Installing on MXLinux Let\u0026rsquo;s try to install Rancher Desktop on one of the Linux distributions, MXLinux.\nTo install, follow the official documentation, add the Rancher Desktop repository, and install Rancher Desktop.\nThere are several packages available for installation on Linux, but since MXLinux is based on Debian(stable), we will use the .deb package.\n1 2 3 4 5 6 7 8 9  # Obtain and register the repository $ curl https://download.opensuse.org/repositories/isv:/Rancher:/stable/deb/Release.key | sudo apt-key add - $ sudo add-apt-repository \u0026#39;deb https://download.opensuse.org/repositories/isv:/Rancher:/stable/deb/ ./\u0026#39; # Updated package list $ sudo apt update # Install Rancher Desktop $ sudo apt install rancher-desktop   This completes the installation of Rancher Desktop. It\u0026rsquo;s very easy.\n About MXLinux\nMXLinux is one of the most popular Linux distributions on DistroWatch.com It is a joint project between antiX and the former MEPIS Linux community and is being developed in Greece and the United States.\n Launching Rancher Desktop Let\u0026rsquo;s run the installed Rancher Desktop. The application itself is very simple.\nGeneral Kubernetes Setting Next, let\u0026rsquo;s take a look at the Kubernetes configuration.\nIn Kubernetes version, you can specify the version of Kubernetes. You can select from v1.23.3, the latest version at the time of writing, to v1.16.7, the oldest version.\nThe Port is set to 6443 by default.\nFor Container runtime, you can choose between containerd and dockerd (moby).\nFor Memory (GB) and CPUs, you can specify the number of memory and CPU cores. If you increase the value up to the red line, a warning message will be displayed as shown below.\nIf you want to clean up your environment once, you can easily reset it by pressing Reset Kubernetes.\nSupporting Utilities In the Supporting Utilities section, you can see the tools that were installed. For Docker, etc., which were already installed, a warning was carefully written.\nImages In Images, you can see the images used by Rancher Desktop. Select Scan from ⋮ in the image list to scan the image for vulnerabilities using Trivy.\nNotice the Image Namespace here. In containerd, the concept of namespace exists as in Kubernetes. So, just as Kubernetes can have namespace, containerd can have namespace as well. In the above figure, the image exists in namespace:k8s.io.\nLet\u0026rsquo;s check the namespace using the nerdctl command. Use nerdctl namespace list to display a list of namespaces.\n1 2 3 4  $ ./.local/bin/nerdctl namespace list NAME CONTAINERS IMAGES VOLUMES buildkit 0 0 0 k8s.io 22 16 0   When you run nerdctl --namespace k8s.io ps, you can see that the image exists in k8s.io.\nYou can also use nerdctl --namespace k8s.io ps to check namespace:k8s.io for Kubernetes containers created with Rancher Desktop.\n1 2 3 4 5 6 7 8 9 10 11 12 13  $ ./.local/bin/nerdctl --namespace k8s.io ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1f642d94e7d5 docker.io/rancher/klipper-lb:v0.3.4 \u0026#34;entry\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr/lb-port-443 2ca56d5f0874 docker.io/rancher/mirrored-library-traefik:2.5.6 \u0026#34;/entrypoint.sh --gl…\u0026#34; 49 minutes ago Up k8s://kube-system/traefik-6bb96f9bd8-cflf4/traefik 2f1c800451cf docker.io/rancher/mirrored-coredns-coredns:1.8.6 \u0026#34;/coredns -conf /etc…\u0026#34; 49 minutes ago Up k8s://kube-system/coredns-5789895cd-wgxlq/coredns 56bd8fba2fda docker.io/rancher/klipper-lb:v0.3.4 \u0026#34;entry\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr/lb-port-80 5908afd18045 docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/coredns-5789895cd-wgxlq 8c16131e6d1b docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 50 minutes ago Up k8s://kube-system/svclb-traefik-svnfr 95b859fec9ed docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/local-path-provisioner-6c79684f77-plbxh a0656b86ab35 docker.io/rancher/local-path-provisioner:v0.0.21 \u0026#34;local-path-provisio…\u0026#34; 49 minutes ago Up k8s://kube-system/local-path-provisioner-6c79684f77-plbxh/local-path-provisioner a5c31106d6d7 docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/traefik-6bb96f9bd8-cflf4 c9417746b27d docker.io/rancher/mirrored-pause:3.6 \u0026#34;/pause\u0026#34; 49 minutes ago Up k8s://kube-system/metrics-server-7cd5fcb6b7-4cbkd ce64e7b0a242 docker.io/rancher/mirrored-metrics-server:v0.5.2 \u0026#34;/metrics-server --c…\u0026#34; 49 minutes ago Up k8s://kube-system/metrics-server-7cd5fcb6b7-4cbkd/metrics-server   Troubleshooting In Troubleshooting, you can enable logging and initialize Rancher Desktop itself.\n検証 Let\u0026rsquo;s try to run a container on Rancher Desktop.\nUse nerdctl Try to start nginx using the nerdctl command. If namespace is not specified, it will be placed in default by default.\nLaunch nginx 1 2 3 4 5 6  $ ./.local/bin/nerdctl run -d -p 9999:80 nginx docker.io/library/nginx:latest: resolved |++++++++++++++++++++++++++++++++++++++| index-sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767: done |++++++++++++++++++++++++++++++++++++++| ... elapsed: 7.2 s total: 54.1 M (7.5 MiB/s) 484e86556e00843200c97b5aa779ba81a9016796e23964e5a0cac27159de444e   Check the status of the container. 1 2 3  $ ./.local/bin/nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 484e86556e00 docker.io/library/nginx:latest \u0026#34;/docker-entrypoint.…\u0026#34; 6 minutes ago Up 0.0.0.0:9999-\u0026gt;80/tcp nginx-484e8   Check the namespace 1 2 3 4 5  $ ./.local/bin/nerdctl namespace list NAME CONTAINERS IMAGES VOLUMES buildkit 0 0 0 default 1 1 0 k8s.io 22 16 0   Check the process of namespace default 1 2 3  $ ./.local/bin/nerdctl --namespace default ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 484e86556e00 docker.io/library/nginx:latest \u0026#34;/docker-entrypoint.…\u0026#34; 11 minutes ago Up 0.0.0.0:9999-\u0026gt;80/tcp nginx-484e8   You can also see the nginx image in namespace:default from Rancher Desktop.\nFinally, try to access localhost:9999 to check if nginx is up and running.\nUse Helm Rancher Desktop also installs Helm at startup, so let\u0026rsquo;s deploy Grafana to Kubernetes using Helm.\nAdd Grafana repository to Helm 1  $ ./.local/bin/helm repo add grafana https://grafana.github.io/helm-charts   Verify that the repository has been added to Helm 1 2 3  $ ./.local/bin/helm repo list NAME URL grafana https://grafana.github.io/helm-charts   View the chart from the added Grafana repository 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  $ ./.local/bin/helm search repo grafana NAME CHART VERSION APP VERSION DESCRIPTION grafana/grafana 6.21.1 8.3.4 The leading tool for querying and visualizing t... grafana/grafana-agent-operator 0.1.5 0.22.0 A Helm chart for Grafana Agent Operator grafana/enterprise-logs 2.0.0 v1.3.0 Grafana Enterprise Logs grafana/enterprise-metrics 1.7.3 v1.6.1 Grafana Enterprise Metrics grafana/fluent-bit 2.3.0 v2.1.0 Uses fluent-bit Loki go plugin for gathering lo... grafana/loki 2.9.1 v2.4.2 Loki: like Prometheus, but for logs. grafana/loki-canary 0.5.1 2.4.1 Helm chart for Grafana Loki Canary grafana/loki-distributed 0.42.0 2.4.2 Helm chart for Grafana Loki in microservices mode grafana/loki-simple-scalable 0.2.0 2.4.2 Helm chart for Grafana Loki in simple, scalable... grafana/loki-stack 2.5.1 v2.1.0 Loki: like Prometheus, but for logs. grafana/promtail 3.10.0 2.4.2 Promtail is an agent which ships the contents o... grafana/tempo 0.13.0 1.3.0 Grafana Tempo Single Binary Mode grafana/tempo-distributed 0.15.0 1.3.0 Grafana Tempo in MicroService mode grafana/tempo-vulture 0.2.0 1.3.0 Grafana Tempo Vulture - A tool to monitor Tempo...   Helm Charts Released 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  $ ./.local/bin/kubectl create namespace monitoring $ ./.local/bin/helm install grafana --namespace monitoring grafana/grafana W0128 03:37:33.477723 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.480386 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.538004 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W0128 03:37:33.538201 218028 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: grafana LAST DEPLOYED: Fri Jan 28 03:37:32 2022 NAMESPACE: monitoring STATUS: deployed REVISION: 1 NOTES: 1. Get your \u0026#39;admin\u0026#39; user password by running: kubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo 2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.monitoring.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace monitoring -l \u0026#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace monitoring port-forward $POD_NAME 3000 3. Login with the password from step 1 and the username: admin ################################################################################# ###### WARNING: Persistence is disabled!!! You will lose your data when ##### ###### the Grafana pod is terminated. ##### #################################################################################   Following the steps above, you should now be able to successfully login to Grafana.\nFinally, use the kubectl command to make sure that Grafana is up and running.\n1 2 3 4 5 6 7 8 9 10 11 12  $ ./.local/bin/kubectl get all --namespace monitoring NAME READY STATUS RESTARTS AGE pod/grafana-6b9d4f7f86-mwb4q 1/1 Running 0 15m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana ClusterIP 10.43.218.65 \u0026lt;none\u0026gt; 80/TCP 15m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1/1 1 1 15m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-6b9d4f7f86 1 1 1 15m   As you can see above, you can easily develop and deploy your application just by running Rancher Desktop.\nImpression This time, I touched Rancher Desktop, which was released v1.0.0, and found it to be a very complete tool. If you are new to containerd or nerdctl, Rancher Desktop is a good opportunity for you.\nLast year, Docker Desktop is now paid became a big news, and Rancher Desktop, but If you switch from Docker Desktop, you will be able to use it without any problems. If you want to use Docker\u0026rsquo;s runtime, you can switch from Rancher Desktop without much trouble.\nAlso, I personally think that the ability to smoothly switch between Kubernetes versions is a big advantage.\nThere is a great possibility that this will become one of the most popular ways to use containers in the future, so I will keep an eye on the future developments.\n","date":"2022-01-27T00:00:00Z","image":"https://example.com/p/running-rancher-desktop-on-mxlinux/featured-image_hua255d1196849d3759ae697794b4989fc_2680_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/running-rancher-desktop-on-mxlinux/","title":"Running Rancher Desktop on MXLinux"},{"content":"Recently, Karpenter graduated from AWS re:Invent with autoscaling of Nodes in Kubernetes clusters.\nIn this article, we\u0026rsquo;ll take a deeper look at it.\nIntroduction This is the 18th day of the [Kubernetes Advent Calendar 2021](\u0026lt;(https://qiita.com/advent-calendar/2021/kubernetes)).\nWhat is Karpenter? Officially described as \u0026ldquo;Just-in-time Nodes for Any Kubernetes Cluster\u0026rdquo;, Karpenter provides the ability to instantly provision new Nodes for unscheduled Pods. The goal is to improve the efficiency and cost of running workloads on Kubernetes clusters.\nKarpenter works as follows.\n Monitor Pods that the Kubernetes scheduler has marked as unschedulable Evaluate the following scheduling constraints as requested by the Pod  Resource Request Node Selector Affinity Tolerant Topology spreading constraints   Provisioning a Node to meet Pod requirements Scheduling a Pod to run on a new Node Deleting a Node when it is no longer needed   How to use Karpenter\nKarpenter will only support AWS as of December 2021.\n Autoscale in Kubernetes Pod There are two ways to scale the Pod.\nHorizontal Pod Auto scaler Horizontal pod scaling is a method of scaling to improve processing performance by increasing the number of pods. User-defined metrics such as CPU, memory, etc. can also be used to make decisions.\nThe number of pods is calculated by the following formula.\nNumber of replicas desired = ceil[\u0026lt;current number of pods\u0026gt; * (\u0026lt;current index value / \u0026lt;target index value\u0026gt;)].\nVertical Pod Auto scaler This is a method of scaling that improves processing performance by increasing the resources available to the pod. In this case, the CPU and memory are used as criteria. It is more like optimizing the resource utilization.\nNode Cluster Auto scaler It is a method of scaling to improve processing performance by increasing the number of worker Nodes. It can also be used in conjunction with horizontal scaling of Pods.\nHow to install Karpenter will be installed on the cluster with Helm Chart. Karpenter also requires IAM Roles for Service Accounts (IRSA).\nCurrently, the utilities required to use Karpenter are as follows\n AWS CLI kubectl  the Kubernetes CLI   eksctl  the CLI for AWS EKS   helm  the package manager for Kubernetes    To learn how to install Karpenter on AWS, please refer to the official document \u0026ldquo;Getting Started with Karpenter on AWS\u0026rdquo;.\nKarpenter\u0026rsquo;s Helm Chart can be found here.\n Installation with Terraform\nKapenter also provides an installation method using Terraform. See here for details.\n An overview diagram is shown in the figure below.\nConfigure the provisioner Karpenter\u0026rsquo;s job is to add Nodes that handle non-schedulable Pods, schedule Pods on those Nodes, and remove the Nodes when they are no longer needed.\nTo configure Karpenter, create a provisioner that defines how Karpenter will manage non-schedulable Pods and timed Nodes.\nThe following is what you need to know about Karpenter provisioners.\nUnschedulable pods Karpenter will only attempt to provision Pods with the status condition Unschedulable=True. This will be set when the kube-scheduler fails to schedule a Pod to an existing capacity.\nProvisioner CR Karpenter defines a custom resource called Provisioner to specify the provisioning configuration.\nEach provisioner manages a separate set of Nodes, but a Pod can be scheduled to any provisioner that supports its scheduling constraints.\nA provisioner contains constraints that affect the Nodes that can be provisioned and the attributes of the Nodes (such as timers for removing Nodes).\nThe following are the resources of the provisioner.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:ttlSecondsUntilExpired:2592000ttlSecondsAfterEmpty:30taints:- key:example.com/special-tainteffect:NoSchedulelabels:billing-team:my-teamrequirements:- key:\u0026#34;node.kubernetes.io/instance-type\u0026#34;operator:Invalues:[\u0026#34;m5.large\u0026#34;,\u0026#34;m5.2xlarge\u0026#34;]- key:\u0026#34;topology.kubernetes.io/zone\u0026#34;operator:Invalues:[\u0026#34;us-west-2a\u0026#34;,\u0026#34;us-west-2b\u0026#34;]- key:\u0026#34;kubernetes.io/arch\u0026#34;operator:Invalues:[\u0026#34;arm64\u0026#34;,\u0026#34;amd64\u0026#34;]- key:\u0026#34;karpenter.sh/capacity-type\u0026#34;operator:Invalues:[\u0026#34;spot\u0026#34;,\u0026#34;on-demand\u0026#34;]provider:{}  The spec.ttlSecondsUntilExpired is the number of seconds the controller will wait before exiting the Node, measured from the time the Node is created. This is useful to eventually implement features like consistent Node upgrades, memory leak protection, and destructive testing. If this field is not set, termination due to expiration will be disabled.\nThe spec.ttlSecondsAfterEmpty is the number of seconds the controller will wait between the time it detects that a Node is empty and the time it tries to remove the Node. A Node is considered empty if there are no Pods scheduled for that Node, except for the daemonset.\nThe spec.requirements constrains the parameters of the provisioned Node. It can be combined with nodeAffinity and nodeSelector. The { In, NotIn } operator is supported to include or exclude values.\nDeprovisioning Node Karpenter deletes the nodes that are no longer needed as follows.\nFinalizer Karpenter will place a finalizer bit in each Node it creates.\nWhen a request to delete these Nodes comes in (such as a TTL or manual Node deletion via kubectl), Karpenter codes the Node, ejects all Pods, terminates the EC2 instance, and deletes the Node object.\nKarpenter handles all the cleanup work required to properly delete the Node.\nNode Expiry When a Node reaches its expiration value (ttlSecondsUntilExpired), it will be ejected and removed from the Pod (even if it is still running a workload).\nEmpty Nodes When the last workload Pod running on a Karpenter-managed Node runs out, that Node will be given an emptiness timestamp. When its \u0026ldquo;Node is empty\u0026rdquo; expiration date (ttlSecondsAfterEmpty) is reached, finalization is triggered.\n About how to remove Node\nFor more information on how Karpenter deletes Nodes, see Details on Node deprovisioning.\n Upgrade Node An easy way to upgrade a Node is to set ttlSecondsUntilExpired, which will expire after a set period of time and be replaced by a newer Node.\nConstraints Because there are no constraints defined by the provisioner or requested by the Pod being deployed, Karpenter is selected from the entire set of features available from the cloud provider. Nodes can be created using any instance type and run in any zone.\nScheduling Karpenter schedules Pods that are marked as unschedulable by the Kubernetes scheduler. After resolving scheduling constraints and startup capacity, it creates a Node and binds the Pod. This stateless approach will help you avoid race conditions and improve performance. If there is a problem with a launched Node, Kubernetes will automatically migrate the Pod to a new Node. When Karpenter launches a Node, it will also allow Kubernetes' scheduler to schedule on it.\nCloud provider Karpenter makes a request to the relevant cloud provider for provisioning a new Node. The first supported cloud provider is AWS, but Karpenter is designed to work with other cloud providers as well. While using the well-known labels of Kubernetes, the provisioner can set a number of values specific to the cloud provider.\nIf you are developing your own provider, you can create it in the repository under pkg/cloudprovider/. The directory structure is as follows. The fake directory is provided as an example for reference.\n1 2 3 4 5 6 7 8  . ├── aws │ ├── apis │ │ └── v1alpha1 │ └── fake ├── fake ├── metrics └── registry   First, you need to create the following files for each cloud provider under pkg/cloudprovider/registry to register them.\n1 2 3 4 5 6 7 8  // +build \u0026lt;YOUR_PROVIDER_NAME\u0026gt; import ( \u0026#34;github.com/aws/karpenter/pkg/cloudprovider/\u0026lt;YOUR_PROVIDER_NAME\u0026gt;\u0026#34; ) func NewCloudProvider() cloudprovider.CloudProvider { return \u0026lt;YOUR_PROVIDER_NAME\u0026gt;.NewCloudProvider() }   You can also create one for each cloud provider under pkg/cloudprovider for your environment. If you check the fake directory, you will find the following files. You can add other necessary information according to your environment.\n1 2 3  . ├── cloudprovider.go └── instancetype.go   Difference from Cluster Auto scaler Like Karpenter, the Kubernetes Cluster Auto scaler is designed to add Nodes when a request comes in to run a Pod that cannot be handled by the current capacity. Cluster Auto scaler is part of the Kubernetes project and is implemented by most of the major Kubernetes cloud providers. By rethinking provisioning, Karpenter provides the following improvements.\nDesigned to take advantage of the flexibility of the cloud Karpenter has the ability to efficiently handle any type of instance available in AWS. Cluster Autoscaler was not originally built with the flexibility to support hundreds of instance types, zones, and purchase options.\nGroup less Node Provisioning Karpenter manages each instance directly, without orchestration mechanisms such as Node groups. This allows you to retry in milliseconds instead of minutes if capacity is not available. It also allows you to take advantage of a variety of instance types, availability zones, and purchasing options without having to create hundreds of Node groups.\nScheduling Implementation Cluster Autoscaler does not bind a Pod to the Node it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the Node comes online. The Node started by Karpenter is bound to its Pod immediately. kubelet does not need to wait for the scheduler or Node to be ready. kubelet does not need to wait for the scheduler or Node to be ready, it can start preparing the container runtime immediately, including pre-pulling images. This can reduce the Node startup latency by a few seconds.\nThoughts In this article, I\u0026rsquo;ve tried to dig a little deeper into Karpenter.\nPersonally, I think it\u0026rsquo;s the same as GKE Autopilot\u0026rsquo;s dynamic Node provisioning process. I think Karpenter is an OSS version of that tool. Like GKE Autopilot, Karpenter observes the specification of non-schedulable pods, computes aggregate resource requests, and sends the requests to an underlying compute service (such as Amazon EC2) that has the capacity required to run all the pods.\nKarpenter also allows you to define custom resources and specify the provisioning configuration for the following Nodes. We found the flexibility to change the configuration to be a significant advantage.\n Instance size/type, topology (zone, etc.) Architecture (arm64, amd64, etc.) Lifecycle type (spot, on-demand, pre-emptive, etc.)  On the other hand, Karpenter can also deprovision a Node when it is no longer needed. This can be determined by setting the Node\u0026rsquo;s expiration date (ttlSecondsUntilExpired) or when the last workload running on a Karpenter provisioned Node has finished (ttlSecondsAfterEmpty). Either of these two events triggers a finalization that codes the Node, ejects the Pod, terminates the underlying compute resource, and deletes the Node object. This deprovisioning feature can also be used to keep Node up-to-date with the latest AMI.\nWith Karpenter, I believe you can offload Node provisioning, autoscaling and upgrading and focus on running your application. Karpenter works with all kinds of Kubernetes applications, but I think it performs especially well in use cases where large amounts of diverse compute resources need to be provisioned and de-provisioned quickly. (Training machine learning models, running simulations, batch jobs with complex financial calculations, etc.)\nCurrently, it only runs on AWS, but we will keep an eye on it in the future. If I have time, I\u0026rsquo;ll try to implement it on other clouds as well.\n","date":"2021-12-16T00:00:00Z","image":"https://example.com/p/karpenter-deep-dive/featured-image_hudfe33c2ee595f9550b80579d6c3a9c6b_13198_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/karpenter-deep-dive/","title":"Karpenter Deep Dive "},{"content":"先日、AWS re:Invent にて Kubernetes クラスターで Node の自動スケーリングをする Karpenter が GA になりました.\n今回は、それについて深堀りしてみます.\nはじめに これは Kubernetes Advent Calendar 2021 18 日目の記事です.\nKarpenter とは Karpenter は、「Just-in-time Nodes for Any Kubernetes Cluster」 と公式で記載されている通り、スケジュールが不能な Pod に対して、瞬時に新しい Node をプロビジョニングする機能を提供します. それにより、Kubernetes クラスター上でワークロードを実行する際の効率とコスト改善をゴールとしています.\nKarpenter の動作は以下の通りになります.\n Kubernetes スケジューラがスケジューリング不能とマークした Pod を監視する Pod から要求された以下のスケジューリング制約の評価  リソース要求 Node セレクタ アフィニティ トレラント トポロジー拡散制約   Pod の要件を満たす Node のプロビジョニング 新しい Node で実行する Pod のスケジューリング Node が不要になったら削除する   Karpenter の利用方法について\nKarpenter は 2021 年 12 月 現在、AWS のみをサポートしています.\n Kubernetes におけるオートスケール Kubernetes には、Pod と Node それぞれにオートスケールする機能が提供されています.\nPod Pod には、以下の 2 種類のスケール方法があります.\n水平スケール (Horizontal Pod Autoscaler) Pod の水平スケールは、Pod 数を増やすことにより処理性能を向上させるスケール方法です. CPU やメモリなど、ユーザが独自に設定したメトリクスなども判断の材料として使えます.\nPod 数は以下の計算式で算出されます.\n希望するレプリカ数 = ceil[\u0026lt;現在の Pod数\u0026gt; * (\u0026lt;現在の指標値 / \u0026lt;ターゲットとする指標値\u0026gt;)]\n垂直スケール (Vertical Pod Autoscaler) Pod が利用可能なリソースを増やすことで処理性能を向上させるスケール方法です. こちらは、CPU やメモリを判断材料に使用します. どちらかと言うとリソース使用率の最適化を行っているイメージです.\nNode Node の水平オートスケーラー (Cluster Autoscaler) ワーカー Node の台数を増やすことによって処理性能を向上させるスケール方法です. Pod の水平スケールなどと連携することも可能です.\nインストール方法 Karpenter は、Helm Chart でクラスタにインストールされます. Karpenter はさらに、IAM Roles for Service Accounts (IRSA)を必要とします.\n現在、Karpenter を使用する際に必要なユーティリティは以下の通りです.\n AWS CLI kubectl  the Kubernetes CLI   eksctl  the CLI for AWS EKS   helm  the package manager for Kubernetes    AWS への Karpenter のインストール方法は、こちらの公式ドキュメントの 「Getting Started with Karpenter on AWS」 を参考にすると良いと思います.\nKarpenter の Helm Chart はこちらから確認することができます.\n Terraform を用いたインストール\nTerraform を使用したインストール方法もあります.詳しくはこちらを参照してください.\n 概要図は下図の通りです。\nプロビジョナーの設定 Karpenter の仕事は、スケジュールできない Pod を処理する Node を追加し、その Node で Pod をスケジュールし、不要になったら Node を削除することです.\nKarpenter を設定するには、Karpenter がスケジューリング不能な Pod と期限付き Node を管理する方法を定義するプロビジョナーを作成します.\n以下は、Karpenter のプロビジョナーについて知っておくと良いと思います.\nUnschedulable pods Karpenter は、ステータス条件 Unschedulable=True を持つ Pod のみをプロビジョニングしようとします. これは、kube-scheduler が既存の容量に Pod をスケジュールすることに失敗したときに設定されます.\nProvisioner CR Karpenter では、プロビジョニング構成を指定するために、Provisioner というカスタムリソースを定義しています.\n各プロビジョナーは個別の Node セットを管理しますが、Pod はそのスケジューリング制約をサポートする任意のプロビジョナーにスケジュールすることができます.\nプロビジョナーには、プロビジョニング可能な Node と Node の属性（Node を削除するためのタイマーなど）に影響を与える制約が含まれています.\n以下が プロビジョナーのリソースになります.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:ttlSecondsUntilExpired:2592000ttlSecondsAfterEmpty:30taints:- key:example.com/special-tainteffect:NoSchedulelabels:billing-team:my-teamrequirements:- key:\u0026#34;node.kubernetes.io/instance-type\u0026#34;operator:Invalues:[\u0026#34;m5.large\u0026#34;,\u0026#34;m5.2xlarge\u0026#34;]- key:\u0026#34;topology.kubernetes.io/zone\u0026#34;operator:Invalues:[\u0026#34;us-west-2a\u0026#34;,\u0026#34;us-west-2b\u0026#34;]- key:\u0026#34;kubernetes.io/arch\u0026#34;operator:Invalues:[\u0026#34;arm64\u0026#34;,\u0026#34;amd64\u0026#34;]- key:\u0026#34;karpenter.sh/capacity-type\u0026#34;operator:Invalues:[\u0026#34;spot\u0026#34;,\u0026#34;on-demand\u0026#34;]provider:{}  spec.ttlSecondsUntilExpiredは、コントローラが Node を終了するまでに待つ秒数で、Node の作成時から計測されます. これは、最終的に一貫した Node アップグレード、メモリリーク保護、破壊テストのような機能を実装するのに便利です. このフィールドが設定されていない場合、有効期限切れによる終了は無効になります.\nspec.ttlSecondsAfterEmptyは、Node が空になったことを検出した時点から、コントローラが Node を削除しようとするまでに待つ秒数です. Node は、デーモンセットを除いて、その Node にスケジュールされている Pod がない場合、空であると見なされます.\nspec.requirementsは、プロビジョニングされた Node のパラメータを制約します. nodeAffinity や nodeSelector と組み合わせることも可能です. 演算子 { In, NotIn } は、値を含めたり除外したりするためにサポートされています.\nNode のデプロビジョニング Karpenter では、不要になった Node を以下のように削除しています.\nFinalizer Karpenter は、作成する各 Node にファイナライザービットを配置します。\nこれらの Node を削除するリクエスト（TTL や手動での kubectl による Node 削除など）が来ると、Karpenter は Node をコード化し、すべての Pod を排出して EC2 インスタンスを終了させ、Node オブジェクトを削除する.\nKarpenter は、Node を適切に削除するために必要なすべてのクリーンアップ作業を処理します.\nNode Expiry Node の有効期限値 (ttlSecondsUntilExpired)に達すると、その Node は（まだワークロードを実行していても）Pod から排出され、削除されます.\nEmpty Nodes Karpenter が管理する Node で稼働している最後のワークロード Pod がなくなると、その Node には emptiness タイムスタンプが付与されます。その「Node が空になる」有効期限 (ttlSecondsAfterEmpty) 達すると、ファイナライズがトリガーされます.\n Node を削除する方法について\nKarpenter が Node を削除する方法の詳細については、Node のデプロビジョニングの詳細を参照してください.\n Node のアップグレード Node をアップグレードする簡単な方法は、ttlSecondsUntilExpired を設定することです。Node は設定された期間後に終了し、より新しい Node と入れ替わります.\n制約条件 プロビジョナーで定義された制約や、デプロイされる Pod から要求された制約がないため、Karpenter はクラウドプロバイダが利用できる機能全体から選択されます. Node は、任意のインスタンスタイプを使用して作成し、任意のゾーンで実行することができます.\nスケジューリング Karpenter は、Kubernetes のスケジューラーが unschedulable とマークした Pod をスケジュールします. スケジューリング制約と起動容量を解決した後、Node を作成し、Pod をバインドします. このステートレスなアプローチは、レースコンディションを回避し、パフォーマンスを向上させるのに役立ちます. 起動した Node に何か問題があれば、Kubernetes は自動的に新しい Node に Pod を移行します.Karpenter が Node を立ち上げると、その Node は Kubernetes のスケジューラーがその上でをスケジュールすることも可能になります.\nクラウドプロバイダー Karpenter は、関連するクラウドプロバイダーに新しい Node のプロビジョニングの要求を行います. 最初にサポートされるクラウドプロバイダーは AWS ですが、Karpenter は他のクラウドプロバイダーでも動作するように設計されています. Kubernetes のよく知られたラベルを使用しながら、プロビジョナーは、クラウドプロバイダーに固有のいくつかの値を設定することができます.\n個人でプロバイダーの開発する場合は、リポジトリのpkg/cloudprovider/配下に作成します. ディレクトリ構造は以下の通りです. fakeディレクトリは、参考例として用意されています.\n1 2 3 4 5 6 7 8  . ├── aws │ ├── apis │ │ └── v1alpha1 │ └── fake ├── fake ├── metrics └── registry   まず、pkg/cloudprovider/registry配下で、クラウドプロバイダー毎に以下の以下のファイルを作成することで登録ができます.\n1 2 3 4 5 6 7 8  // +build \u0026lt;YOUR_PROVIDER_NAME\u0026gt; import ( \u0026#34;github.com/aws/karpenter/pkg/cloudprovider/\u0026lt;YOUR_PROVIDER_NAME\u0026gt;\u0026#34; ) func NewCloudProvider() cloudprovider.CloudProvider { return \u0026lt;YOUR_PROVIDER_NAME\u0026gt;.NewCloudProvider() }   また、pkg/cloudprovider配下で、クラウドプロバイダーごとに環境に合わせて作成します. fake ディレクトリを確認すると以下のファイルが用意されています. その他の必要な情報は環境に合わせて追加すると良いです.\n1 2 3  . ├── cloudprovider.go └── instancetype.go   Cluster Autoscaler との違い Karpenter と同様に、Kubernetes Cluster Autoscaler は、現在のキャパシティでは対応できない Pod の実行要求が来たときに、Node を追加するように設計されています. Cluster Autoscaler は Kubernetes プロジェクトの一部であり、ほとんどの主要な Kubernetes クラウドプロバイダーが実装しています. プロビジョニングを見直すことで、Karpenter は以下の改善を提供しています.\nクラウドの柔軟性を活かした設計 Karpenter は、AWS で利用できるあらゆる種類のインスタンスに効率的に対応できる能力を備えています. Cluster Autoscaler は、もともと何百ものインスタンスタイプ、ゾーン、購入オプションに対応できるような柔軟性を持って構築されたものではありません.\nグループレスの Node プロビジョニング Karpenter は、Node グループのようなオーケストレーションの仕組みを使わずに、各インスタンスを直接管理します. これにより、キャパシティが利用できない場合、数分ではなくミリ秒単位で再試行することができます. また、何百もの Node グループを作成することなく、多様なインスタンスタイプ、アベイラビリティゾーン、および購入オプションを活用することができます.\nスケジューリングの実施 Cluster Autoscaler は、作成した Node に Pod をバインドしません. その代わり、Node がオンラインになった後に同じスケジューリング決定を行うために kube-scheduler に依存します.Karpenter が起動した Node には、すぐにその Pod がバインドされます。kubelet` はスケジューラーや Node の準備が整うのを待つ必要がありません. イメージの事前プルも含め、コンテナランタイムの準備をすぐに開始できます.これにより、Node の起動レイテンシを数秒短縮することができます.\n所感 今回は、Karpenter について少し深堀りしてみました.\n個人的には、GKE Autopilot の動的 Node プロビジョニングプロセスと同じなのかなと思っています. Karpenter はそのツールの OSS 版と言えると思います. GKE Autopilot と同様に、Karpenter はスケジューリング不能な Pod の仕様を観測し、集約されたリソース要求を計算し、すべての Pod の実行に必要な容量を持つ基礎的な計算サービス（Amazon EC2 など）に要求を送信します.\nまた、Karpenter では、カスタムリソースを定義して、以下の Node のプロビジョニング構成を指定することができます. 構成を柔軟に変更できる点は、かなり大きいメリットだと感じました.\n インスタンスサイズ/タイプ、トポロジー(ゾーンなど) アーキテクチャ(arm64、amd64 など) ライフサイクルタイプ(スポット、オンデマンド、プリエンプティブなど)  一方、Karpenter は、Node が不要になった場合、デプロビジョンを行うこともできます. これは、Node の有効期限設定 (ttlSecondsUntilExpired) または Karpenter プロビジョニングされた Node 上で実行されている最後のワークロードが終了したとき (ttlSecondsAfterEmpty) に決定することができます. この 2 つのイベントのどちらかがトリガーとなり、Node をコード化し、Pod を排出し、基盤となるコンピュートリソースを終了させ、Node オブジェクトを削除するファイナライゼーションが行われます. このデプロビジョニング機能は、Node を最新の AMI で最新の状態に保つためにも使用できます.\nKarpenter を使えば、Node のプロビジョニング、オートスケール、アップグレードをオフロードして、アプリケーションの実行に集中することができると思います. Karpenter はあらゆる種類の Kubernetes アプリケーションで動作しますが、特に、大量の多様な計算リソースを迅速にプロビジョニングおよびデプロビジョニングする必要があるユースケースで優れたパフォーマンスを発揮すると思います. (機械学習モデルのトレーニング、シミュレーションの実行、複雑な金融計算を行うバッチジョブなど)\n現在は、AWS のみでしか動作しませんが今後の動向には注目していきたいと思います. また、時間があれば他クラウドへの実装などもしてみようと思います.\n","date":"2021-12-16T00:00:00Z","image":"https://example.com/p/karpenter-%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/featured-image_hudfe33c2ee595f9550b80579d6c3a9c6b_13198_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/karpenter-%E3%82%92%E8%AA%BF%E3%81%B9%E3%81%A6%E3%81%BF%E3%81%9F/","title":"Karpenter を調べてみた"},{"content":"Google Authenticator の機種変更時の引き継ぎ方法について説明します.\nまた、個人的に推奨している Microsoft Authenticator についても解説します.\n概要 近年、セキュリティの強化として2 段階認証の導入が推奨され、多くの方が SMS 認証や Google Authenticator などの認証アプリケーションをスマートフォンなどにインストールしているかと思います.\nしかし、認証アプリは機種変更などをした際に正しい引き継ぎ手順を踏まなければ、認証アプリにログインできなくなります.それによって苦汁を舐めた人たちも多くいるのではないでしょうか.\nそのような事態にならないためにも、正しい手順を理解し、スムーズに引き継げるようにしましょう.\nそもそも 2 段階認証って？ 2 段階認証はその名の通り、「2 度の認証を行う」ことです. 従来の 1 つの認証方法を使うよりはるかにセキュアに管理する事ができます.\n2 段階認証には、様々な方法があります.\n SMS, メールなどを用いる場合  認証サーバなどで特定の文字列などが発行され、送信されます. ユーザはそれを画面上で入力します.     2 段階認証アプリケーションを用いる場合  アプリケーション上でワンタイムのセキュリティコードが自動的に生成され、そのコードを画面上に入力します.    ※ 2 段階認証アプリは別名、TOTP (Time-based One-Time Password) アプリとも呼ばれます.\n2 段階認証を扱う上での注意点 いくらセキュアな機能だとしても注意するべき点はいくつかあります.\n2 段階目の認証として最も利用されているのはスマートフォンだと思います. スマートフォンの電話番号宛に SMS が届き、そのコードを利用したり、キャリアメールアドレスに送られるコードを利用したりするため、スマートフォンの紛失・盗難時にログインできなくなります.\nこれは、ワンタイムパスワードを用いる場合も同じことが言えます.\nGoogle Authenticator を利用している場合は、スマートフォンにインストールして利用しているため、紛失・盗難時には同じくログインすることができません.\nまた、スマートフォンの機種を変更した場合も引き継ぎを正しく行っていなければアプリケーションをインストールしても復元することができません.\nこれらを回避する方法としては以下の通りです.\n 複数の認証方法を設定しておく  SMS + 2 段階認証アプリケーション 2 段階認証アプリケーション + メール etc\u0026hellip;   引き継ぎの際、2 段階認証の設定を無効化(解除)しておく  引き継ぎ方法 (Google Authenticator) Google Authenticator を新しいデバイスへ引き継ぐ方法について解説します.\n以前までは、アプリケーションに登録している 2 段階認証を全て無効化する必要がありましたが、現在は引き継ぐ前のデバイスでアカウント情報をエクスポートし、 それを新しいデバイスでインポートすることでアカウントを移行することができます.\nそれぞれのデバイスでの手順は以下の通りです.\niOS   移行前のデバイス\n Google Authenticator を起動 起動したアプリケーションの右上のメニューボタン...をタップし、「アカウントのエクスポート」を選択 エクスポート画面で、右下「続行」ボタンをタップ 指紋認証、FaceID、または端末に設定している PIN（パスコード）を入力して本人確認を実行 移行させるアカウントを選択して、「エクスポート」をタップ 移行用 QR コードが表示されたら準備完了    新しいデバイス\n Google Authenticator を起動し、「開始」をタップ 左下「既存のアカウントをインポートしますか？」をタップ 右下「QR コードをスキャン」をタップ エクスポート時に作成した移行用 QR コードを読み込む    Android   移行前のデバイス\n Google Authenticator を起動 起動したアプリケーションの右上のメニューボタン...をタップし、「アカウントの移行」を選択 エクスポート画面で、右下「アカウントのエクスポート」ボタンをタップ 本人確認を実行 「次へ」をタップ 移行用 QR コードが表示されたら準備完了    新しいデバイス\n Google Authenticator を起動し、「使ってみる」をタップ 左下「既存のアカウントをインポートしますか？」をタップ 右下「QR コードをスキャン」をタップ エクスポート時に作成した移行用 QR コードを読み込む    上記の手順を実行後、「インポート完了」と表示されれば引き継ぎは完了です.\nMicrosoft Authenticator の利用 上記では Google Authenticator を解説しましたが、私はMicrosoft Authenticatorを使用しています.\n理由としては以下の通りです.\n バックアップ機能が搭載されていない 登録されているアプリケーションを並び替えると UI が壊れる  同一アプリケーションが複数表示される (誤って消すと両方とも消える)   スマートフォン・アプリケーションでしか利用できない  非常にシンプルなのですが、UI が壊れるのも耐え難く、最大の理由としては「バックアップ機能が搭載されていない」という点です. 万が一、デバイスが起動しなくなった際の再設定などの手間を考えると、あまり使いやすいとは言えません.\n一方で、Microsoft Authenticator だと、Microsoft アカウントをアプリと同期することでバックアップが可能です. 詳しい方法に関しては、こちらを参照ください.\n※ iOS 版では iCloud でバックアップを行うため、iCloud のアカウントが必要となります.\nしかし、Microsoft Authenticator も Google Authenticator と同じく、スマートフォン・アプリケーションでしか利用できません. 「PC でも使用したい！」と言う方は、Twilio社のAuthyを使用することをおすすめします.\n※ Authy を利用する場合は、別途 Authy のアカウントを作成する必要があります.\nまとめ 今回は、2 段階認証の仕組みと、認証アプリケーションの引き継ぎ方法などについて解説しました.\n2 段階認証アプリケーションを使用することで、アカウントの情報をよりセキュアに管理することができる反面、きちんと管理する必要があります. 特に 2 段階認証設定後のバックアップコードの管理には十分、注意してください.\n実体が無いとは言え、重要な資産なので、この記事を参考に大切に保管しましょう.\n","date":"2021-10-04T00:00:00Z","image":"https://example.com/p/google-authenticator-%E3%81%8B%E3%82%89%E3%81%AE%E5%BC%95%E3%81%8D%E7%B6%99%E3%81%8E%E6%96%B9%E6%B3%95/featured-image_hue5d9e6be829627395a8e9ef9ad7b7d16_204404_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/google-authenticator-%E3%81%8B%E3%82%89%E3%81%AE%E5%BC%95%E3%81%8D%E7%B6%99%E3%81%8E%E6%96%B9%E6%B3%95/","title":"Google Authenticator からの引き継ぎ方法"},{"content":"個人で使用しているデバイスの詳細情報について説明します.\nMacbook Pro (13-inch, 2019)    項目 内容     OS macOS 12.2 21D49 x86_64   CPU Intel i7-8569U (8) @ 2.80GHz   GPU Intel Iris Plus Graphics 655   Memory 16 GB 2133 MHz LPDDR3   Terminal iTerm2   Terminal Font FiraCodeNerdFontComplete-Regular   Shell zsh 5.8    PC 使用している PC には、3 つの OS がインストールされています。\n共通 (Desktop)    項目 内容     Motherboard ROG STRIX H370-F GAMING (ASUSTeK COMPUTER INC.)   CPU Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz   GPU NVIDIA GeForce GTX 1060 6GB (MSI Co., Ltd.)   Memory DDR4-2666MHz 8×4GB (Corsair Inc.)   Storage(OS) Crucial MX500 500×2GB (Micron Technology, Inc.)   Storage Western Digital Blue: 6TB, Green: 3×2TB (Western Digital Corporation)    WindowsOS    項目 内容     OS Windows 11 Pro (64bit)   Terminal Windows Console   Terminal Font FiraCode   Shell PowerShell v7.1.4    Linux (1)    項目 内容     OS Manjaro Linux x86_64   Terminal xfce4-terminal   Terminal Font FiraCode Nerd Font   Shell zsh 5.8    Linux (2)    項目 内容     OS MX x86_64   Terminal xfce4-terminal   Terminal Font Lilex Nerd Font Mono Medium   Shell zsh 5.8   ","date":"2021-09-28T00:00:00Z","image":"https://example.com/p/%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%83%87%E3%83%90%E3%82%A4%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/featured-image_hu73aa009c4655c053a851e705e92c6897_259024_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%83%87%E3%83%90%E3%82%A4%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/","title":"使用しているデバイスについて"},{"content":"以下のプレイリストを現在、作成しています.\nTPO に合わせて様々なジャンルの音楽をお楽しみ下さい.\nJ-HipHop  J-HipHop (Chill)  HipHop  J-Rock  Rock  Dubstep, Bass  Kawaii  House  EDM ","date":"2021-06-08T00:00:00Z","image":"https://example.com/p/%E4%BF%BA%E3%81%AE-spotify-%E3%83%97%E3%83%AC%E3%82%A4%E3%83%AA%E3%82%B9%E3%83%88/featured-image_hu1ab211327d5ace997f347121b19693c5_93430_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://example.com/p/%E4%BF%BA%E3%81%AE-spotify-%E3%83%97%E3%83%AC%E3%82%A4%E3%83%AA%E3%82%B9%E3%83%88/","title":"俺の Spotify プレイリスト"}]